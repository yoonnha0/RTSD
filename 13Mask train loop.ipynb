{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409409be",
   "metadata": {
    "id": "409409be"
   },
   "source": [
    "# Создание классов датасетов российских дорожных знаков (RTSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb7e84e",
   "metadata": {
    "id": "fdb7e84e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import random\n",
    "import cv2\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41dcca9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41dcca9b",
    "outputId": "dc34a7ee-d3cf-483f-f8b9-d318e960041d"
   },
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !kaggle datasets download -d watchman/rtsd-dataset\n",
    "# !unzip rtsd-dataset.zip\n",
    "# !rm rtsd-dataset.zip\n",
    "# !cp -r rtsd-frames/rtsd-frames/ .\n",
    "# !rm -r rtsd-frames/rtsd-frames/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d5cefa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41d5cefa",
    "outputId": "50af7bc2-3734-4ed3-8949-672c97f3ffe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec485fc2",
   "metadata": {
    "id": "ec485fc2"
   },
   "source": [
    "### DATASET DETECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b122bfda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "b122bfda",
    "outputId": "1399d742-1b55-4b80-872e-34f83ac8aad5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "n_epochs = 10\n",
    "batch_size = 12\n",
    "classes = 2\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eefd7ed",
   "metadata": {
    "id": "5eefd7ed"
   },
   "source": [
    "# Mask RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2aef6d5",
   "metadata": {
    "id": "a2aef6d5"
   },
   "outputs": [],
   "source": [
    "# функция загрузки датасета minmax\n",
    "def dataset_1_mask(path, name_f):\n",
    "  \n",
    "  with open(os.path.join(path, name_f), 'r') as f:\n",
    "    anno = json.load(f)\n",
    "\n",
    "    obj1 = anno.get('images')\n",
    "    df1 = pd.json_normalize(obj1)\n",
    "    obj2 = anno.get('annotations')\n",
    "    df2 = pd.json_normalize(obj2)\n",
    "    obj3 = anno.get('categories')\n",
    "    df3 = pd.json_normalize(obj3)\n",
    "    t = df2.merge(df3.set_index('id'), left_on='category_id',right_index=True)\n",
    "    df= t.merge(df1.set_index('id'), left_on='image_id',right_index=True)\n",
    "    df_mask = df[['file_name', 'width', 'height', 'category_id', 'bbox', 'id', 'image_id', 'area', 'iscrowd', 'name']]\n",
    "    df_mask.category_id = 1\n",
    "    df_mask['xmin'] = df_mask.bbox.apply(lambda x: x[0])\n",
    "    df_mask['ymin'] = df_mask.bbox.apply(lambda x: x[1])\n",
    "    df_mask['xmax'] = df_mask.bbox.apply(lambda x: x[0]+x[2])\n",
    "    df_mask['ymax'] = df_mask.bbox.apply(lambda x: x[1]+x[3])\n",
    "    df_mask = df_mask[['file_name', 'width', 'height', 'category_id', 'xmin', 'ymin', 'xmax', 'ymax', 'bbox', 'id', 'image_id', 'area', 'iscrowd', 'name' ]]\n",
    "    \n",
    "  \n",
    "  return df_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cbdf586",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0cbdf586",
    "outputId": "f2fc416f-6677-4f50-b5b0-6d5084684293"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-828ffd0d-de16-4a62-9778-4185c605722d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>category_id</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>bbox</th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>area</th>\n",
       "      <th>iscrowd</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rtsd-frames/autosave01_02_2012_09_13_33.jpg</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1</td>\n",
       "      <td>649</td>\n",
       "      <td>376</td>\n",
       "      <td>667</td>\n",
       "      <td>394</td>\n",
       "      <td>[649, 376, 18, 18]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>324</td>\n",
       "      <td>0</td>\n",
       "      <td>2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rtsd-frames/autosave01_02_2012_09_13_34.jpg</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1</td>\n",
       "      <td>671</td>\n",
       "      <td>356</td>\n",
       "      <td>691</td>\n",
       "      <td>377</td>\n",
       "      <td>[671, 356, 20, 21]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rtsd-frames/autosave01_02_2012_09_13_35.jpg</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1</td>\n",
       "      <td>711</td>\n",
       "      <td>332</td>\n",
       "      <td>738</td>\n",
       "      <td>358</td>\n",
       "      <td>[711, 332, 27, 26]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rtsd-frames/autosave01_02_2012_09_13_36.jpg</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1</td>\n",
       "      <td>764</td>\n",
       "      <td>290</td>\n",
       "      <td>801</td>\n",
       "      <td>326</td>\n",
       "      <td>[764, 290, 37, 36]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1332</td>\n",
       "      <td>0</td>\n",
       "      <td>2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rtsd-frames/autosave01_02_2012_09_13_36.jpg</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1</td>\n",
       "      <td>684</td>\n",
       "      <td>384</td>\n",
       "      <td>701</td>\n",
       "      <td>401</td>\n",
       "      <td>[684, 384, 17, 17]</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>1_23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-828ffd0d-de16-4a62-9778-4185c605722d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-828ffd0d-de16-4a62-9778-4185c605722d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-828ffd0d-de16-4a62-9778-4185c605722d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                     file_name  width  height  category_id  \\\n",
       "0  rtsd-frames/autosave01_02_2012_09_13_33.jpg   1280     720            1   \n",
       "1  rtsd-frames/autosave01_02_2012_09_13_34.jpg   1280     720            1   \n",
       "2  rtsd-frames/autosave01_02_2012_09_13_35.jpg   1280     720            1   \n",
       "3  rtsd-frames/autosave01_02_2012_09_13_36.jpg   1280     720            1   \n",
       "4  rtsd-frames/autosave01_02_2012_09_13_36.jpg   1280     720            1   \n",
       "\n",
       "   xmin  ymin  xmax  ymax                bbox  id  image_id  area  iscrowd  \\\n",
       "0   649   376   667   394  [649, 376, 18, 18]   0         0   324        0   \n",
       "1   671   356   691   377  [671, 356, 20, 21]   1         1   420        0   \n",
       "2   711   332   738   358  [711, 332, 27, 26]   2         2   702        0   \n",
       "3   764   290   801   326  [764, 290, 37, 36]   3         3  1332        0   \n",
       "4   684   384   701   401  [684, 384, 17, 17]   4         3   289        0   \n",
       "\n",
       "   name  \n",
       "0   2_1  \n",
       "1   2_1  \n",
       "2   2_1  \n",
       "3   2_1  \n",
       "4  1_23  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3 = dataset_1_mask('.', 'train_anno.json')\n",
    "val3 = dataset_1_mask('.', 'val_anno.json')\n",
    "train3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3deccb1",
   "metadata": {
    "id": "e3deccb1"
   },
   "outputs": [],
   "source": [
    "# class PennFudanDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, root, transforms):\n",
    "#         self.root = root\n",
    "#         self.transforms = transforms\n",
    "#         # load all image files, sorting them to\n",
    "#         # ensure that they are aligned\n",
    "#         self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "#         self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # load images and masks\n",
    "#         img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "#         mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "#         # note that we haven't converted the mask to RGB,\n",
    "#         # because each color corresponds to a different instance\n",
    "#         # with 0 being background\n",
    "#         mask = Image.open(mask_path)\n",
    "#         # convert the PIL Image into a numpy array\n",
    "#         mask = np.array(mask)\n",
    "#         # instances are encoded as different colors\n",
    "#         obj_ids = np.unique(mask)\n",
    "#         # first id is the background, so remove it\n",
    "#         obj_ids = obj_ids[1:]\n",
    "\n",
    "#         # split the color-encoded mask into a set\n",
    "#         # of binary masks\n",
    "#         masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "#         # get bounding box coordinates for each mask\n",
    "#         num_objs = len(obj_ids)\n",
    "#         boxes = []\n",
    "#         for i in range(num_objs):\n",
    "#             pos = np.where(masks[i])\n",
    "#             xmin = np.min(pos[1])\n",
    "#             xmax = np.max(pos[1])\n",
    "#             ymin = np.min(pos[0])\n",
    "#             ymax = np.max(pos[0])\n",
    "#             boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "#         # convert everything into a torch.Tensor\n",
    "#         boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "#         # there is only one class\n",
    "#         labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "#         masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "#         image_id = torch.tensor([idx])\n",
    "#         area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "#         # suppose all instances are not crowd\n",
    "#         iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "#         target = {}\n",
    "#         target[\"boxes\"] = boxes\n",
    "#         target[\"labels\"] = labels\n",
    "#         target[\"masks\"] = masks\n",
    "#         target[\"image_id\"] = image_id\n",
    "#         target[\"area\"] = area\n",
    "#         target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "#         if self.transforms is not None:\n",
    "#             img, target = self.transforms(img, target)\n",
    "\n",
    "#         return img, target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af73c33",
   "metadata": {
    "id": "7af73c33"
   },
   "outputs": [],
   "source": [
    "class RTSD_dataset_my_m(Dataset):\n",
    "    def __init__(self, path_img, df, transforms=None):\n",
    "        super().__init__()\n",
    "        self.path_img = path_img\n",
    "        self.transforms = transforms\n",
    "        self.df = df\n",
    "#         self.df = self.df.groupby('file_name', as_index=False).agg(list)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        name_img = self.df.loc[index,'file_name']\n",
    "        img = Image.open(os.path.join(self.path_img, name_img))\n",
    "        \n",
    "#         def create_bb_array(x):\n",
    "#             \"\"\"Генерируем массив bounding box'a из столбца train_df\"\"\"\n",
    "#             return np.array([x[5],x[4],x[7],x[6]])\n",
    "        \n",
    "        \n",
    "        img = img.resize((720, 1280))\n",
    "        \n",
    "        \n",
    "        image_width = self.df.loc[index,'width'] #img.shape[1]\n",
    "        image_height = self.df.loc[index,'height'] #img.shape[0]\n",
    "        \n",
    "        xmin = (self.df.loc[index][4]/image_width)*1280\n",
    "        xmax = (self.df.loc[index][6]/image_width)*1280\n",
    "        ymin = (self.df.loc[index][5]/image_height)*720\n",
    "        ymax = (self.df.loc[index][7]/image_height)*720\n",
    "        \n",
    "        bb = np.array([int(ymin), int(xmin), int(ymax), int(xmax)])\n",
    "              \n",
    "        \n",
    "        def create_mask(bb):\n",
    "            \"\"\"Создаем маску для bounding box'a такого же шейпа как и изображение\"\"\"\n",
    "            rows,cols = 720, 1280\n",
    "            Y = np.zeros((rows, cols))\n",
    "            bb = bb.astype(np.int)\n",
    "            Y[bb[0]:bb[2], bb[1]:bb[3]] = 1.\n",
    "            return Y\n",
    "\n",
    "\n",
    "        def mask_to_bb(Y):\n",
    "            \"\"\"Конвертируем маску Y в bounding box'a, принимая 0 как фоновый ненулевой объект \"\"\"\n",
    "            cols, rows = np.nonzero(Y)\n",
    "            if len(cols) == 0: \n",
    "                return np.zeros(4, dtype=np.float32)\n",
    "            top_row = np.min(rows)\n",
    "            left_col = np.min(cols)\n",
    "            bottom_row = np.max(rows)\n",
    "            right_col = np.max(cols)\n",
    "            return np.array([left_col, top_row, right_col, bottom_row], dtype=np.float32)\n",
    "\n",
    "        mask = create_mask(bb)\n",
    "        \n",
    "        \n",
    "        obj_ids = np.unique(mask)\n",
    "        \n",
    "        # Number of Bounding Boxes\n",
    "#         obj_ids = np.array(list(range(len(boxes))))\n",
    "        obj_ids = obj_ids[1:]\n",
    "        ## Split mask into seperate mask\n",
    "        masks = mask == obj_ids[:, None,None]\n",
    "\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "    \n",
    "        boxes = []\n",
    "        boxes.append(mask_to_bb(mask))\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones(1, dtype=torch.int64)\n",
    "        img_id = torch.tensor([self.df.loc[index, 'image_id']])\n",
    "\n",
    "#         areas = torch.as_tensor(self.df.loc[index,'area'], dtype=torch.float32)\n",
    "        areas = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros(1).to(torch.int64)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"masks\"] = masks\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "\n",
    "        img = img / 255\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc6558f",
   "metadata": {
    "id": "0bc6558f"
   },
   "outputs": [],
   "source": [
    "# In my case, just added ToTensor\n",
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a703ac0",
   "metadata": {
    "id": "2a703ac0"
   },
   "outputs": [],
   "source": [
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb9a0dcf",
   "metadata": {
    "id": "fb9a0dcf"
   },
   "outputs": [],
   "source": [
    "df3 = RTSD_dataset_my_m('./', train3, transforms=get_transform())\n",
    "df33 = RTSD_dataset_my_m('./', val3, transforms=get_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055288b2",
   "metadata": {
    "id": "055288b2"
   },
   "outputs": [],
   "source": [
    "data_loader_train3 = torch.utils.data.DataLoader(df3,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn,\n",
    "                                          drop_last = True)\n",
    "\n",
    "data_loader_val3 = torch.utils.data.DataLoader(df33,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn,\n",
    "                                          drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85e45fab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85e45fab",
    "outputId": "06ed6773-8540-481b-de07-845de2b0bba3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95492"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e7068f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e7068f5",
    "outputId": "78094f42-6f9c-4c38-ed7f-81307bd6b2fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.2303e-04, 1.6917e-04, 2.1530e-04,  ..., 9.5348e-04,\n",
       "           1.0458e-03, 1.3072e-03],\n",
       "          [1.2303e-04, 1.6917e-04, 2.1530e-04,  ..., 9.3810e-04,\n",
       "           1.4302e-03, 1.7070e-03],\n",
       "          [1.2303e-04, 1.5379e-04, 1.9992e-04,  ..., 9.6886e-04,\n",
       "           2.1838e-03, 2.4298e-03],\n",
       "          ...,\n",
       "          [3.0757e-05, 3.0757e-05, 3.0757e-05,  ..., 6.3053e-04,\n",
       "           5.8439e-04, 4.9212e-04],\n",
       "          [1.5379e-05, 1.5379e-05, 1.5379e-05,  ..., 1.8454e-04,\n",
       "           1.8454e-04, 1.5379e-04],\n",
       "          [1.5379e-05, 1.5379e-05, 1.5379e-05,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "         [[4.3060e-04, 4.7674e-04, 5.0750e-04,  ..., 9.6886e-04,\n",
       "           1.0458e-03, 1.3072e-03],\n",
       "          [4.3060e-04, 4.6136e-04, 5.0750e-04,  ..., 9.5348e-04,\n",
       "           1.4302e-03, 1.7070e-03],\n",
       "          [4.3060e-04, 4.4598e-04, 4.9212e-04,  ..., 9.8424e-04,\n",
       "           2.1992e-03, 2.4298e-03],\n",
       "          ...,\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.6121e-04,\n",
       "           7.8431e-04, 6.6128e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2295e-04,\n",
       "           2.9220e-04, 2.6144e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5379e-05,\n",
       "           1.5379e-05, 4.6136e-05]],\n",
       " \n",
       "         [[5.5363e-04, 5.6901e-04, 6.1515e-04,  ..., 9.9962e-04,\n",
       "           1.0765e-03, 1.3379e-03],\n",
       "          [5.5363e-04, 5.6901e-04, 6.1515e-04,  ..., 9.8424e-04,\n",
       "           1.4610e-03, 1.7378e-03],\n",
       "          [5.3825e-04, 5.5363e-04, 5.9977e-04,  ..., 1.0150e-03,\n",
       "           2.2299e-03, 2.4606e-03],\n",
       "          ...,\n",
       "          [4.6136e-05, 4.6136e-05, 4.6136e-05,  ..., 8.4583e-04,\n",
       "           7.5356e-04, 5.9977e-04],\n",
       "          [3.0757e-05, 3.0757e-05, 3.0757e-05,  ..., 3.0757e-04,\n",
       "           2.6144e-04, 1.9992e-04],\n",
       "          [3.0757e-05, 3.0757e-05, 3.0757e-05,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00]]]),\n",
       " {'boxes': tensor([[363., 677., 394., 759.]]),\n",
       "  'labels': tensor([1]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'image_id': tensor([686]),\n",
       "  'area': tensor([2542.]),\n",
       "  'iscrowd': tensor([0])})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.__getitem__(968)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3da1fae9",
   "metadata": {
    "id": "3da1fae9"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56f7d918",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56f7d918",
    "outputId": "fb415021-3a60-4726-ada1-db63eb0dca94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
      "100%|██████████| 170M/170M [00:01<00:00, 102MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(2).to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0987a1e",
   "metadata": {
    "id": "e0987a1e"
   },
   "outputs": [],
   "source": [
    "images,targets = next(iter(data_loader_train3))\n",
    "images = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "output = model(images,targets)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffa5f3",
   "metadata": {
    "id": "67ffa5f3",
    "outputId": "83ae41e4-3f63-40a4-a593-2403ae791715"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(0.9984, grad_fn=<NllLossBackward0>),\n",
       " 'loss_box_reg': tensor(0.0020, grad_fn=<DivBackward0>),\n",
       " 'loss_mask': tensor(1.4168, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'loss_objectness': tensor(0.1257, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'loss_rpn_box_reg': tensor(0.0023, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bea91b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4bea91b",
    "outputId": "e423b40d-548d-4ef6-876e-db82881eb832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 0.9116367101669312\n",
      "epoch: 0, step: 200, loss: 0.3091042637825012\n",
      "epoch: 0, step: 400, loss: 26.220123291015625\n",
      "epoch: 0, step: 600, loss: 49.15089797973633\n",
      "epoch: 0, step: 800, loss: 35.286895751953125\n",
      "epoch: 0, step: 1000, loss: 451.1973571777344\n",
      "epoch: 0, step: 1200, loss: 53048.8046875\n",
      "epoch: 0, step: 1400, loss: 1119.857421875\n",
      "epoch: 0, step: 1600, loss: 114.10011291503906\n",
      "epoch: 0, step: 1800, loss: 118877.203125\n",
      "epoch: 0, step: 2000, loss: 0.39057427644729614\n",
      "epoch: 0, step: 2200, loss: 0.3609306216239929\n",
      "epoch: 0, step: 2400, loss: 0.317417711019516\n",
      "epoch: 0, step: 2600, loss: 0.3197692036628723\n",
      "epoch: 0, step: 2800, loss: 0.30734506249427795\n",
      "epoch: 0, step: 3000, loss: 0.29751622676849365\n",
      "epoch: 0, step: 3200, loss: 0.23442599177360535\n",
      "epoch: 0, step: 3400, loss: 0.23490169644355774\n",
      "epoch: 0, step: 3600, loss: 0.17649583518505096\n",
      "epoch: 0, step: 3800, loss: 0.18605004251003265\n",
      "epoch: 0, step: 4000, loss: 0.2401772141456604\n",
      "epoch: 0, step: 4200, loss: 0.19198063015937805\n",
      "epoch: 0, step: 4400, loss: 0.1609044373035431\n",
      "epoch: 0, step: 4600, loss: 0.1351201981306076\n",
      "epoch: 0, step: 4800, loss: 0.3302312195301056\n",
      "epoch: 0, step: 5000, loss: 0.16762499511241913\n",
      "epoch: 0, step: 5200, loss: 0.20430269837379456\n",
      "epoch: 0, step: 5400, loss: 0.14172053337097168\n",
      "epoch: 0, step: 5600, loss: 0.13558454811573029\n",
      "epoch: 0, step: 5800, loss: 0.2336781769990921\n",
      "epoch: 0, step: 6000, loss: 0.4612039625644684\n",
      "epoch: 0, step: 6200, loss: 0.16371233761310577\n",
      "epoch: 0, step: 6400, loss: 0.21991553902626038\n",
      "epoch: 0, step: 6600, loss: 0.16947388648986816\n",
      "epoch: 0, step: 6800, loss: 0.1353047639131546\n",
      "epoch: 0, step: 7000, loss: 0.22067703306674957\n",
      "epoch: 0, step: 7200, loss: 0.15871413052082062\n",
      "epoch: 0, step: 7400, loss: 0.16333965957164764\n",
      "epoch: 0, step: 7600, loss: 0.19473139941692352\n",
      "epoch: 0, step: 7800, loss: 0.10967080295085907\n",
      "Эпоха train: 0 Итераций: 7956 train loss: 1.2862322362240024e-05\n",
      "epoch: 0, step: 0, loss: 0.11753062903881073\n",
      "epoch: 0, step: 200, loss: 0.10908665508031845\n",
      "epoch: 0, step: 400, loss: 0.14019402861595154\n",
      "epoch: 0, step: 600, loss: 0.12919236719608307\n",
      "Потрачено 463.3 минут на 0 эпоху\n",
      "Эпоха val: 0 Итераций: 737 val loss: 0.0003879493256598671\n",
      "epoch: 1, step: 0, loss: 0.153619647026062\n",
      "epoch: 1, step: 200, loss: 0.38801413774490356\n",
      "epoch: 1, step: 400, loss: 0.13468223810195923\n",
      "epoch: 1, step: 600, loss: 0.17881722748279572\n",
      "epoch: 1, step: 800, loss: 0.7658960223197937\n",
      "epoch: 1, step: 1000, loss: 0.14143900573253632\n",
      "epoch: 1, step: 1200, loss: 0.1441171020269394\n",
      "epoch: 1, step: 1400, loss: 0.26008880138397217\n",
      "epoch: 1, step: 1600, loss: 0.12524160742759705\n",
      "epoch: 1, step: 1800, loss: 0.17324307560920715\n",
      "epoch: 1, step: 2000, loss: 0.32536599040031433\n",
      "epoch: 1, step: 2200, loss: 0.23766526579856873\n",
      "epoch: 1, step: 2400, loss: 0.11643936485052109\n",
      "epoch: 1, step: 2600, loss: 0.14326681196689606\n",
      "epoch: 1, step: 2800, loss: 0.16943617165088654\n",
      "epoch: 1, step: 3000, loss: 0.13810883462429047\n",
      "epoch: 1, step: 3200, loss: 0.3619287610054016\n",
      "epoch: 1, step: 3400, loss: 0.3791382908821106\n",
      "epoch: 1, step: 3600, loss: 0.1430303007364273\n",
      "epoch: 1, step: 3800, loss: 0.24914304912090302\n",
      "epoch: 1, step: 4000, loss: 0.17486821115016937\n",
      "epoch: 1, step: 4200, loss: 0.1088344007730484\n",
      "epoch: 1, step: 4400, loss: 0.17144227027893066\n",
      "epoch: 1, step: 4600, loss: 0.19648675620555878\n",
      "epoch: 1, step: 4800, loss: 0.09823832660913467\n",
      "epoch: 1, step: 5000, loss: 0.2559731602668762\n",
      "epoch: 1, step: 5200, loss: 0.08547595143318176\n",
      "epoch: 1, step: 5400, loss: 0.5037123560905457\n",
      "epoch: 1, step: 5600, loss: 0.14882436394691467\n",
      "epoch: 1, step: 5800, loss: 0.12704816460609436\n",
      "epoch: 1, step: 6000, loss: 0.2597235143184662\n",
      "epoch: 1, step: 6200, loss: 0.4462793469429016\n",
      "epoch: 1, step: 6400, loss: 0.3037225604057312\n",
      "epoch: 1, step: 6600, loss: 0.12889166176319122\n",
      "epoch: 1, step: 6800, loss: 0.18832701444625854\n",
      "epoch: 1, step: 7000, loss: 0.11456504464149475\n",
      "epoch: 1, step: 7200, loss: 0.17558154463768005\n",
      "epoch: 1, step: 7400, loss: 0.4116343557834625\n",
      "epoch: 1, step: 7600, loss: 0.1466178148984909\n",
      "epoch: 1, step: 7800, loss: 0.4281032085418701\n",
      "Эпоха train: 1 Итераций: 7956 train loss: 3.0601467392203676e-05\n",
      "epoch: 1, step: 0, loss: 0.11632194370031357\n",
      "epoch: 1, step: 200, loss: 0.10742218792438507\n",
      "epoch: 1, step: 400, loss: 0.14030902087688446\n",
      "epoch: 1, step: 600, loss: 0.12863798439502716\n",
      "Потрачено 471.6 минут на 1 эпоху\n",
      "Эпоха val: 1 Итераций: 737 val loss: 0.00039299077062464473\n",
      "epoch: 2, step: 0, loss: 0.16491307318210602\n",
      "epoch: 2, step: 200, loss: 0.16109433770179749\n",
      "epoch: 2, step: 400, loss: 0.2830009162425995\n",
      "epoch: 2, step: 600, loss: 0.12032448500394821\n",
      "epoch: 2, step: 800, loss: 0.3236772418022156\n",
      "epoch: 2, step: 1000, loss: 0.14083217084407806\n",
      "epoch: 2, step: 1200, loss: 0.1182502806186676\n",
      "epoch: 2, step: 1400, loss: 0.18840770423412323\n",
      "epoch: 2, step: 1600, loss: 0.3623671531677246\n",
      "epoch: 2, step: 1800, loss: 0.30374661087989807\n",
      "epoch: 2, step: 2000, loss: 0.21316562592983246\n",
      "epoch: 2, step: 2200, loss: 0.17429903149604797\n",
      "epoch: 2, step: 2400, loss: 0.14642764627933502\n",
      "epoch: 2, step: 2600, loss: 0.2740435004234314\n",
      "epoch: 2, step: 2800, loss: 0.17826513946056366\n",
      "epoch: 2, step: 3000, loss: 0.17797337472438812\n",
      "epoch: 2, step: 3200, loss: 0.1171189546585083\n",
      "epoch: 2, step: 3400, loss: 0.10224599391222\n",
      "epoch: 2, step: 3600, loss: 0.25551488995552063\n",
      "epoch: 2, step: 3800, loss: 0.1518310159444809\n",
      "epoch: 2, step: 4000, loss: 0.14224615693092346\n",
      "epoch: 2, step: 4200, loss: 0.17917582392692566\n",
      "epoch: 2, step: 4400, loss: 0.13620784878730774\n"
     ]
    }
   ],
   "source": [
    "loss1_train = []\n",
    "loss1_val = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for i, batch in enumerate(data_loader_train3):\n",
    "        optimizer.zero_grad()\n",
    "        loss_train = 0\n",
    "        imgs, targets = batch\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#         try:\n",
    "        loss_dict = model(imgs, targets)\n",
    "#         except:\n",
    "#             print(batch)\n",
    "#         else:\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_train += losses.item()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {losses.item()}')\n",
    "    \n",
    "    loss11_train = (loss_train / i)\n",
    "    loss1_train.append(loss11_train)\n",
    "    \n",
    "    print('Эпоха train:', epoch,'Итераций:', i, 'train loss:', (loss_train / i))\n",
    "    \n",
    "    for i, batch in enumerate(data_loader_val3):\n",
    "        optimizer.zero_grad()\n",
    "        loss_val = 0\n",
    "        imgs, targets = batch\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.no_grad():\n",
    "              loss_dict = model(imgs, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_val += losses.item()\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {losses.item()}')\n",
    "    \n",
    "    loss11_val = (loss_val / i)\n",
    "    loss1_val.append(loss11_val)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Потрачено {round((end - start) / 60, 1)} минут на {epoch} эпоху\")\n",
    "    print('Эпоха val:', epoch, 'Итераций:', i, 'val loss:', (loss_val / i))\n",
    "    \n",
    "        \n",
    "    torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                    'loss_train': loss1_train,\n",
    "                    'loss_val': loss1_val,\n",
    "                    }, f'./chkpt_model3_d_{epoch}.pth')\n",
    "    \n",
    "    torch.save(model.state_dict(), f'./chkpt_m3_d_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb5145",
   "metadata": {
    "id": "6acb5145"
   },
   "source": [
    "#### Визуализация данных (Аргументация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ae880",
   "metadata": {
    "id": "031ae880"
   },
   "outputs": [],
   "source": [
    "# mask\n",
    "def show_tranformed_image0(train_loader):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(18, 10)\n",
    "    if len(train_loader) > 0:\n",
    "        for i in range(1):\n",
    "            _, targets = next(iter(train_loader))\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            boxes = targets[i]['masks']#.cpu().numpy().astype(np.int32)\n",
    "            for box in boxes:\n",
    "                plt.imshow(box, cmap='gray')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed738f",
   "metadata": {
    "id": "26ed738f",
    "outputId": "2dee3f4e-0288-40df-886a-3c5a4d2666ee"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "show_tranformed_image0(data_loader_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbdd9ea",
   "metadata": {
    "id": "0fbdd9ea"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
