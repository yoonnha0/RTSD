{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45641a58",
   "metadata": {},
   "source": [
    "# Создание моделей обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd2ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import random\n",
    "import cv2\n",
    "from pycocotools.coco import COCO\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torchvision.models import mobilenet_v3_large, resnet152\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291891b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id = -1\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 6\n",
    "classes = 2\n",
    "classes1 = 156\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd2834",
   "metadata": {},
   "source": [
    "### models DETECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2717e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_model, classes):\n",
    "    if 'resnet50_fpn_v2' in n_model:\n",
    "        model =torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights='FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT')\n",
    "    elif 'resnet50_fpn' in n_model:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    else:\n",
    "        print(\"нет такой модели \")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5921c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f47c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка модели, чтобы продолжить обучение и предсказать после предыдущей эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "136b47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath, map_location=device)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['loss_val']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e694d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp1(checkpoint_fpath, model):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath, map_location=device)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa4e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_detect_1 \n",
    "model1 = build_model('resnet50_fpn_v2', classes).to(device)\n",
    "params1 = [p for p in model1.parameters() if p.requires_grad]\n",
    "optimizer1 = torch.optim.Adam(params1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c694bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_detect_mask\n",
    "model1 = get_model_instance_segmentation(2).to(device)\n",
    "params1 = [p for p in model1.parameters() if p.requires_grad]\n",
    "optimizer1 = torch.optim.Adam(params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127abe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_detect_backbone \n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b2f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_detect_2\n",
    "model2 = build_model('resnet50_fpn', classes).to(device)\n",
    "params2 = [p for p in model1.parameters() if p.requires_grad]\n",
    "optimizer2 = torch.optim.Adam(params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdcf3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model('resnet50_fpn', classes).to(device)\n",
    "params2 = [p for p in model1.parameters() if p.requires_grad]\n",
    "optimizer2 = torch.optim.Adam(params2, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeae86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask RNN\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_model_instance_segmentation(2).to(device)\n",
    "params1 = [p for p in model1.parameters() if p.requires_grad]\n",
    "optimizer1 = torch.optim.Adam(params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc352de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decafe0d",
   "metadata": {},
   "source": [
    "### models CLASSIFIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ee36c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_clf(classes):\n",
    "    model = mobilenet_v3_large(weights='MobileNet_V3_Large_Weights.IMAGENET1K_V2')\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.classifier[3] = nn.Linear(in_features=model.classifier[3].in_features, out_features=classes)\n",
    "#     model.classifier[3] = nn.Linear(in_features=1280, out_features=classes)\n",
    "    for param in  model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f47b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_clf_1 \n",
    "model = build_model_clf(classes1).to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_clf(classes1).to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "803cb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(classes):\n",
    "    model = resnet152(weights='ResNet152_Weights.IMAGENET1K_V2')\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.fc = nn.Linear(model.fc.in_features, classes)\n",
    "    \n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53bcf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to C:\\Users\\150ho/.cache\\torch\\hub\\checkpoints\\resnet152-f82ba261.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff8638dd17a480b895fc738ecb72cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_clf_11 \n",
    "model = create_model(classes1).to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6749334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_clf_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb19018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates the Residual block of ResNet.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, use_1x1conv=True, strides=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        if self.conv3:\n",
    "            inputs = self.conv3(inputs)\n",
    "        x += inputs\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d67c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_block(\n",
    "    input_channels,\n",
    "    output_channels, \n",
    "    num_residuals,\n",
    "):\n",
    "        resnet_block = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0:\n",
    "                resnet_block.append(ResidualBlock(input_channels, output_channels,\n",
    "                                    use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                resnet_block.append(ResidualBlock(output_channels, output_channels))\n",
    "        return resnet_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d425d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes=156):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(16), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(*create_resnet_block(16, 32, 2))\n",
    "        self.block3 = nn.Sequential(*create_resnet_block(32, 64, 2))\n",
    "        self.block4 = nn.Sequential(*create_resnet_block(64, 128, 2))\n",
    "        self.block5 = nn.Sequential(*create_resnet_block(128, 256, 2))\n",
    "        self.linear = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        x = self.linear(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21c89f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = CustomResNet().to(device)\n",
    "optim = torch.optim.Adam(model_conv.parameters())\n",
    "loss_f = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12428a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = CustomResNet().to(device)\n",
    "optim = torch.optim.Adam(model_conv.parameters(), lr=0.0001, weight_decay=0.0005)\n",
    "loss_f = nn.CrossEntropyLoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
