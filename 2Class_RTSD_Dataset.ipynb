{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409409be",
   "metadata": {},
   "source": [
    "# Создание классов датасетов российских дорожных знаков (RTSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb7e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import random\n",
    "import cv2\n",
    "# from pycocotools.coco import COCO\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41dcca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !kaggle datasets download -d watchman/rtsd-dataset\n",
    "# !unzip rtsd-dataset.zip\n",
    "# !rm rtsd-dataset.zip\n",
    "# !cp -r rtsd-frames/rtsd-frames/ .\n",
    "# !rm -r rtsd-frames/rtsd-frames/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d5cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec485fc2",
   "metadata": {},
   "source": [
    "### DATASET DETECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b122bfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id = -1\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "n_epochs = 5\n",
    "batch_size = 6\n",
    "classes = 2\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916296e4",
   "metadata": {},
   "source": [
    "#### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a318fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция загрузки датасета в pd\n",
    "def dataset(path, name_f):\n",
    "  \n",
    "  with open(os.path.join(path, name_f), 'r') as f:\n",
    "    anno = json.load(f)\n",
    "\n",
    "    obj1 = anno.get('images')\n",
    "    df1 = pd.json_normalize(obj1)\n",
    "    obj2 = anno.get('annotations')\n",
    "    df2 = pd.json_normalize(obj2)\n",
    "    obj3 = anno.get('categories')\n",
    "    df3 = pd.json_normalize(obj3)\n",
    "    t = df2.merge(df3.set_index('id'), left_on='category_id',right_index=True)\n",
    "    df= t.merge(df1.set_index('id'), left_on='image_id',right_index=True)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2991419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_anno = dataset('.', 'train_anno.json')\n",
    "df_val_anno = dataset('.', 'val_anno.json')\n",
    "df_train_anno_reduced = dataset('.', 'train_anno_reduced.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приведем все знаки к одному классу\n",
    "df_train_anno_1 = df_train_anno.copy()\n",
    "df_train_anno_1['category_id'] = 1\n",
    "\n",
    "df_val_anno_1 = df_val_anno.copy()\n",
    "df_val_anno_1['category_id'] = 1\n",
    "\n",
    "df_train_anno_reduced_1 = df_train_anno_reduced.copy()\n",
    "df_train_anno_reduced_1['category_id'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61dac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделим df_train_anno на train / test\n",
    "random_state=33\n",
    "df_test_1 = df_train_anno_1.sample(frac = 0.2, random_state=random_state)\n",
    "df_train_1 = df_train_anno_1.drop(df_test_1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a89ea080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTSD_dataset_my(Dataset):\n",
    "    def __init__(self, path_img, df, transforms=None):\n",
    "        super().__init__()\n",
    "        self.path_img = path_img\n",
    "        self.transforms = transforms\n",
    "        self.df = df\n",
    "        self.df = self.df.groupby('file_name', as_index=False).agg(list)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        name_img = self.df.loc[index,'file_name']\n",
    "        img = Image.open(os.path.join(self.path_img, name_img))\n",
    "      \n",
    "        boxes = []\n",
    "        for b in self.df.loc[index,'bbox']:\n",
    "            bb = [b[0], b[1], b[0] + b[2], b[1] + b[3]]\n",
    "            boxes.append(bb)   \n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.Tensor(self.df.loc[index, 'category_id']).to(torch.int64)\n",
    "        img_id = torch.tensor([self.df.loc[index, 'image_id'][0]])\n",
    "        \n",
    "        areas = []\n",
    "        for i in self.df.loc[index,'area']:\n",
    "            areas.append(self.df.loc[index,'area'])\n",
    "        areas = torch.as_tensor(areas[0], dtype=torch.float32)\n",
    "       \n",
    "        iscrowd = torch.zeros(len(self.df.loc[index,'iscrowd'])).to(torch.int64)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        img = img / 255\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc6558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my case, just added ToTensor\n",
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c09292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_a():\n",
    "             return transforms.Compose([transforms.Resize((512,512)),\n",
    "                    transforms.RandomChoice([transforms.Compose([transforms.RandomPerspective(), transforms.ToTensor()]),\n",
    "                    transforms.Compose([transforms.ColorJitter(), transforms.ToTensor()]),\n",
    "                    transforms.Compose([transforms.RandomResizedCrop((512,512)),transforms.ToTensor()]),\n",
    "                    transforms.Compose([transforms.ToTensor(),transforms.RandomErasing()])])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7062c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = RTSD_dataset_my('./', df_train_anno_reduced_1,transforms=get_transform())\n",
    "df11 = RTSD_dataset_my('./', df_val_anno_1,transforms=get_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa060640",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = RTSD_dataset_my('./', df_train_anno_reduced_1,transforms=get_transform_a())\n",
    "df22 = RTSD_dataset_my('./', df_val_anno_1,transforms=get_transform_a())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train1 = torch.utils.data.DataLoader(df1,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn,\n",
    "                                          drop_last = True)\n",
    "\n",
    "data_loader_train2 = torch.utils.data.DataLoader(df2,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "data_loader_val1 = torch.utils.data.DataLoader(df11,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn,\n",
    "                                          drop_last = True)\n",
    "\n",
    "data_loader_val2 = torch.utils.data.DataLoader(df22,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee7d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tranformed_image(data_loader_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad32fe",
   "metadata": {},
   "source": [
    "#### COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51046f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anno_1_class(anno_file):\n",
    " \n",
    "    with open(os.path.join('.', anno_file), 'r') as read_file:\n",
    "        anno_json = json.load(read_file)\n",
    "    read_file.close()\n",
    "\n",
    "    for i in range(len(anno_json['annotations'])):\n",
    "        anno_json['annotations'][i]['category_id'] = 1    \n",
    "    \n",
    "    anno_json['categories'] = [{'id': 1, 'name': 'sign'}]\n",
    "    \n",
    "    anno_file_new = anno_file.split('.')[0] + '_1.json'\n",
    "    \n",
    "    with open(os.path.join('.', anno_file_new), 'w') as f:\n",
    "        json.dump(anno_json, f)\n",
    "    f.close()\n",
    "\n",
    "    return anno_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anno_train_test_split(anno_file):\n",
    "    \n",
    "    with open(os.path.join('.', anno_file), 'r') as file1:\n",
    "        anno_json1 = json.load(file1)\n",
    "    \n",
    "    x1 = int(len(anno_json1['images'])*0.2)\n",
    "    file_train = anno_file.split('.')[0] + '_train.json'\n",
    "          \n",
    "    del anno_json1['images'][:x1]\n",
    "    \n",
    "    with open(os.path.join('.', file_train), 'w') as f1:\n",
    "                        json.dump(anno_json1, f1)\n",
    "    \n",
    "    \n",
    "    with open(os.path.join('.', anno_file), 'r') as file2:\n",
    "            anno_json2 = json.load(file2)\n",
    "    \n",
    "    \n",
    "    x2 = int(len(anno_json2['images'])*0.2)\n",
    "    y2 = int(len(anno_json2['images']))\n",
    "    \n",
    "    file_test= anno_file.split('.')[0] + '_test.json'\n",
    "    \n",
    "    del anno_json2['images'][x2:y2]\n",
    "            \n",
    "    with open(os.path.join('.', file_test), 'w') as f2:\n",
    "                        json.dump(anno_json2, f2)  \n",
    "            \n",
    "    return anno_json1, anno_json2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anno_1 = anno_1_class('train_anno.json')\n",
    "val_anno_1 = anno_1_class('val_anno.json')\n",
    "train_anno_reduced_1 = anno_1_class('train_anno_reduced.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = anno_train_test_split('train_anno_1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e0b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTSD_dataset_coco(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[index]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "    \n",
    "        \n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i][\"bbox\"][0]\n",
    "            ymin = coco_annotation[i][\"bbox\"][1]\n",
    "            xmax = xmin + coco_annotation[i][\"bbox\"][2]\n",
    "            ymax = ymin + coco_annotation[i][\"bbox\"][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([img_id])\n",
    "        # Size of bbox (Rectangular)\n",
    "        areas = []\n",
    "        for i in range(num_objs):\n",
    "            areas.append(coco_annotation[i][\"area\"])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        # Iscrowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            \n",
    "        img = img / 255\n",
    "\n",
    "    \n",
    "        return img, my_annotation\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb581b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '.'\n",
    "annotation = './train_anno_reduced_1.json'\n",
    "\n",
    "# create own Dataset\n",
    "df_1 = RTSD_dataset_coco(root=root,\n",
    "                          annotation=annotation,\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "# df_1.__getitem__(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac83174",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '.'\n",
    "annotation = './val_anno_1.json'\n",
    "\n",
    "# create own Dataset\n",
    "df_11 = RTSD_dataset_coco(root=root,\n",
    "                          annotation=annotation,\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "# df_11.__getitem__(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029798ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '.'\n",
    "annotation = './train_anno_reduced_1.json'\n",
    "\n",
    "# create own Dataset\n",
    "df_2 = RTSD_dataset_coco(root=root,\n",
    "                          annotation=annotation,\n",
    "                          transforms=get_transform_a()\n",
    "                          )\n",
    "# df_2.__getitem__(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '.'\n",
    "annotation = './val_anno_1.json'\n",
    "\n",
    "# create own Dataset\n",
    "df_22 = RTSD_dataset_coco(root=root,\n",
    "                          annotation=annotation,\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "# df_22.__getitem__(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92faee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train_1 = torch.utils.data.DataLoader(df_1,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn,\n",
    "                                          drop_last = True)\n",
    "\n",
    "data_loader_train_2 = torch.utils.data.DataLoader(df_2,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "data_loader_val_1 = torch.utils.data.DataLoader(df_11,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn,\n",
    "                                          drop_last = True)\n",
    "\n",
    "data_loader_val_2 = torch.utils.data.DataLoader(df_22,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724de8f4",
   "metadata": {},
   "source": [
    "#### Визуализация аргументации данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tranformed_image(train_loader):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(18, 10)\n",
    "    if len(train_loader) > 0:\n",
    "        for i in range(1):\n",
    "            images, targets = next(iter(train_loader))\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "            sample = images[i].permute(1, 2, 0).detach().cpu().numpy()\n",
    "            sample = sample * 255\n",
    "            ax.imshow(sample)\n",
    "            for box in boxes:\n",
    "                      x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
    "                      rect = patches.Rectangle((x, y),\n",
    "                                              width, height,\n",
    "                                              fc ='none', \n",
    "                                              ec ='g',\n",
    "                                              lw = 2)\n",
    "                      ax.add_patch(rect)\n",
    "            plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c43ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tranformed_image(data_loader_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tranformed_image(data_loader_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf5d5b6",
   "metadata": {},
   "source": [
    "### DATASET CLASSIFIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = -1\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "n_epochs = 5\n",
    "batch_size = 6\n",
    "classes1 = 156\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd6454",
   "metadata": {},
   "source": [
    "#### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87106992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTSD_dataset_clf_my(Dataset):\n",
    "    def __init__(self, path_img, df, transforms=None):\n",
    "        super().__init__()\n",
    "        self.path_img = path_img\n",
    "        self.transforms = transforms\n",
    "        self.df = df\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        name_img = self.df.loc[index,'file_name']\n",
    "        boxes = self.df.loc[index,'bbox']\n",
    "        bb = [boxes[0], boxes[1], boxes[0] + boxes[2], boxes[1] + boxes[3]]\n",
    "        imgs = Image.open(os.path.join(self.path_img, name_img))\n",
    "        imgs = imgs.crop(bb)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            imgs = self.transforms(imgs)\n",
    "\n",
    "        imgs = imgs / 255\n",
    "\n",
    "        targets = torch.tensor(self.df.loc[index, 'category_id'])\n",
    "\n",
    "\n",
    "#         return imgs, targets\n",
    "        return {\n",
    "                'images': imgs,\n",
    "                'targets': targets}\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform1():\n",
    "    return transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_a1():\n",
    "             return transforms.Compose([transforms.Resize((512,512)),\n",
    "                    transforms.RandomChoice([transforms.Compose([transforms.RandomPerspective(), transforms.ToTensor()]),\n",
    "                    transforms.Compose([transforms.ColorJitter(), transforms.ToTensor()]),\n",
    "                    transforms.Compose([transforms.RandomResizedCrop((512,512)),transforms.ToTensor()]),\n",
    "                    transforms.Compose([transforms.ToTensor(),transforms.RandomErasing()])])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09312ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = RTSD_dataset_clf_my('./', df_train_anno_reduced, transforms=get_transform1())\n",
    "df112 = RTSD_dataset_clf_my('./', df_val_anno, transforms=get_transform1())\n",
    "df222 = RTSD_dataset_clf_my('./', df_train_anno_reduced, transforms=get_transform_a1())\n",
    "df122 = RTSD_dataset_clf_my('./', df_val_anno, transforms=get_transform_a1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train12 = torch.utils.data.DataLoader(df12,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, drop_last = True)\n",
    "\n",
    "data_loader_val12 = torch.utils.data.DataLoader(df112,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, drop_last = True)\n",
    "data_loader_train22 = torch.utils.data.DataLoader(df222,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, drop_last = True)\n",
    "\n",
    "data_loader_val22 = torch.utils.data.DataLoader(df122,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f12b4f",
   "metadata": {},
   "source": [
    "#### Визуализация аргументации данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tranformed_image2(train_loader):\n",
    "    if len(train_loader) > 0:\n",
    "        for i in range(1):\n",
    "            data = next(iter(train_loader))\n",
    "            images, targets = data['images'], data['targets']\n",
    "            images = list(image.to(device) for image in images)\n",
    "            sample = images[i].permute(1, 2, 0).detach().cpu().numpy()\n",
    "            sample = sample * 255\n",
    "            targets = [t.to(device) for t in targets]\n",
    "            targets = targets[i].item()\n",
    "            \n",
    "            plt.imshow(sample)\n",
    "            plt.title(targets)\n",
    "            plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tranformed_image2(data_loader_train22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tranformed_image12(train_loader):\n",
    "            figure = plt.figure(figsize=(10, 8))\n",
    "            cols, rows = 3, 3\n",
    "            if len(train_loader) > 0:\n",
    "                  for i in range(1, cols * rows + 1):\n",
    "                              for j in range(1):\n",
    "                                  data = next(iter(train_loader))\n",
    "                                  images, targets = data['images'], data['targets']\n",
    "                                  targets = targets[j].item()\n",
    "                                  sample = images[j].permute(1, 2, 0).detach().cpu().numpy()\n",
    "                                  sample = sample * 255\n",
    "                                  figure.add_subplot(rows, cols, i)\n",
    "                                                    \n",
    "                                  plt.imshow(sample)\n",
    "                                  \n",
    "                                  plt.title(targets)\n",
    "                                  plt.axis(\"off\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tranformed_image12(data_loader_train22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
