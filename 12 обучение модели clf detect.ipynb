{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PAupiJmeZ5V2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAupiJmeZ5V2",
    "outputId": "b6452c27-605e-42b6-dee0-79fb5ee099ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kKqyXMKjQGsK",
   "metadata": {
    "id": "kKqyXMKjQGsK"
   },
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !kaggle datasets download -d watchman/rtsd-dataset\n",
    "# !unzip rtsd-dataset.zip\n",
    "# !rm rtsd-dataset.zip\n",
    "# !cp -r rtsd-frames/rtsd-frames/ .\n",
    "# !rm -r rtsd-frames/rtsd-frames/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PI19hYNxnL1L",
   "metadata": {
    "id": "PI19hYNxnL1L"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import random\n",
    "import cv2\n",
    "from pycocotools.coco import COCO\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "from torchvision.models import mobilenet_v3_large, resnet152\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e176c87",
   "metadata": {},
   "source": [
    "### CLF_mobilenet_v3_large на большой выборке train_anno + Аугментация данных + class не знак"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R4tL-08inL30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "R4tL-08inL30",
    "outputId": "1db5f412-fc9a-4aa9-b0d7-e35442945b5b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "classes = 2\n",
    "classes1 = 157\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TY-P9lQdnL6r",
   "metadata": {
    "id": "TY-P9lQdnL6r"
   },
   "outputs": [],
   "source": [
    "# функция загрузки датасета в pd\n",
    "def dataset(path, name_f):\n",
    "  \n",
    "  with open(os.path.join(path, name_f), 'r') as f:\n",
    "    anno = json.load(f)\n",
    "\n",
    "    obj1 = anno.get('images')\n",
    "    df1 = pd.json_normalize(obj1)\n",
    "    obj2 = anno.get('annotations')\n",
    "    df2 = pd.json_normalize(obj2)\n",
    "    obj3 = anno.get('categories')\n",
    "    df3 = pd.json_normalize(obj3)\n",
    "    t = df2.merge(df3.set_index('id'), left_on='category_id',right_index=True)\n",
    "    df= t.merge(df1.set_index('id'), left_on='image_id',right_index=True)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_46Ph_4FnL9o",
   "metadata": {
    "id": "_46Ph_4FnL9o"
   },
   "outputs": [],
   "source": [
    "df_train_anno = dataset('.', 'train_anno.json')\n",
    "df_val_anno = dataset('.', 'val_anno.json')\n",
    "df_train_anno_reduced = dataset('.', 'train_anno_reduced.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6l_2r13Z75RU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6l_2r13Z75RU",
    "outputId": "e78a90eb-290c-46d9-b01a-d31961e5ac98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fihC7oQsnMAc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fihC7oQsnMAc",
    "outputId": "1c15d245-72f5-43df-fe48-7e2afd9ab98f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5b507c3a-b1a7-49fc-91e9-e9e9e76defa9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>area</th>\n",
       "      <th>bbox</th>\n",
       "      <th>iscrowd</th>\n",
       "      <th>name</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>324</td>\n",
       "      <td>[649, 376, 18, 18]</td>\n",
       "      <td>0</td>\n",
       "      <td>2_1</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>rtsd-frames/autosave01_02_2012_09_13_33.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "      <td>[671, 356, 20, 21]</td>\n",
       "      <td>0</td>\n",
       "      <td>2_1</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>rtsd-frames/autosave01_02_2012_09_13_34.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>702</td>\n",
       "      <td>[711, 332, 27, 26]</td>\n",
       "      <td>0</td>\n",
       "      <td>2_1</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>rtsd-frames/autosave01_02_2012_09_13_35.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1332</td>\n",
       "      <td>[764, 290, 37, 36]</td>\n",
       "      <td>0</td>\n",
       "      <td>2_1</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>rtsd-frames/autosave01_02_2012_09_13_36.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>289</td>\n",
       "      <td>[684, 384, 17, 17]</td>\n",
       "      <td>0</td>\n",
       "      <td>1_23</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>rtsd-frames/autosave01_02_2012_09_13_36.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95487</th>\n",
       "      <td>70506</td>\n",
       "      <td>38149</td>\n",
       "      <td>149</td>\n",
       "      <td>2100</td>\n",
       "      <td>[1007, 307, 35, 60]</td>\n",
       "      <td>0</td>\n",
       "      <td>6_8_1</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>rtsd-frames/autosave16_10_2012_08_36_42_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95488</th>\n",
       "      <td>70507</td>\n",
       "      <td>38150</td>\n",
       "      <td>149</td>\n",
       "      <td>3150</td>\n",
       "      <td>[1088, 296, 45, 70]</td>\n",
       "      <td>0</td>\n",
       "      <td>6_8_1</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>rtsd-frames/autosave16_10_2012_08_36_43_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95489</th>\n",
       "      <td>102242</td>\n",
       "      <td>57700</td>\n",
       "      <td>154</td>\n",
       "      <td>1519</td>\n",
       "      <td>[712, 329, 31, 49]</td>\n",
       "      <td>0</td>\n",
       "      <td>7_14</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>rtsd-frames/autosave24_10_2013_11_23_19_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95490</th>\n",
       "      <td>102243</td>\n",
       "      <td>57701</td>\n",
       "      <td>154</td>\n",
       "      <td>2340</td>\n",
       "      <td>[767, 306, 39, 60]</td>\n",
       "      <td>0</td>\n",
       "      <td>7_14</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>rtsd-frames/autosave24_10_2013_11_23_19_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95491</th>\n",
       "      <td>102245</td>\n",
       "      <td>57703</td>\n",
       "      <td>154</td>\n",
       "      <td>16912</td>\n",
       "      <td>[1143, 101, 112, 151]</td>\n",
       "      <td>0</td>\n",
       "      <td>7_14</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>rtsd-frames/autosave24_10_2013_11_23_20_1.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95492 rows × 10 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b507c3a-b1a7-49fc-91e9-e9e9e76defa9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5b507c3a-b1a7-49fc-91e9-e9e9e76defa9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5b507c3a-b1a7-49fc-91e9-e9e9e76defa9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "           id  image_id  category_id   area                   bbox  iscrowd  \\\n",
       "0           0         0            1    324     [649, 376, 18, 18]        0   \n",
       "1           1         1            1    420     [671, 356, 20, 21]        0   \n",
       "2           2         2            1    702     [711, 332, 27, 26]        0   \n",
       "3           3         3            1   1332     [764, 290, 37, 36]        0   \n",
       "4           4         3            2    289     [684, 384, 17, 17]        0   \n",
       "...       ...       ...          ...    ...                    ...      ...   \n",
       "95487   70506     38149          149   2100    [1007, 307, 35, 60]        0   \n",
       "95488   70507     38150          149   3150    [1088, 296, 45, 70]        0   \n",
       "95489  102242     57700          154   1519     [712, 329, 31, 49]        0   \n",
       "95490  102243     57701          154   2340     [767, 306, 39, 60]        0   \n",
       "95491  102245     57703          154  16912  [1143, 101, 112, 151]        0   \n",
       "\n",
       "        name  width  height                                      file_name  \n",
       "0        2_1   1280     720    rtsd-frames/autosave01_02_2012_09_13_33.jpg  \n",
       "1        2_1   1280     720    rtsd-frames/autosave01_02_2012_09_13_34.jpg  \n",
       "2        2_1   1280     720    rtsd-frames/autosave01_02_2012_09_13_35.jpg  \n",
       "3        2_1   1280     720    rtsd-frames/autosave01_02_2012_09_13_36.jpg  \n",
       "4       1_23   1280     720    rtsd-frames/autosave01_02_2012_09_13_36.jpg  \n",
       "...      ...    ...     ...                                            ...  \n",
       "95487  6_8_1   1280     720  rtsd-frames/autosave16_10_2012_08_36_42_2.jpg  \n",
       "95488  6_8_1   1280     720  rtsd-frames/autosave16_10_2012_08_36_43_0.jpg  \n",
       "95489   7_14   1280     720  rtsd-frames/autosave24_10_2013_11_23_19_1.jpg  \n",
       "95490   7_14   1280     720  rtsd-frames/autosave24_10_2013_11_23_19_2.jpg  \n",
       "95491   7_14   1280     720  rtsd-frames/autosave24_10_2013_11_23_20_1.jpg  \n",
       "\n",
       "[95492 rows x 10 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa = df_train_anno.copy().reset_index()\n",
    "dfa = dfa.drop(['index'],axis=1)\n",
    "dfa.iloc[10:100, 4] = dfa.iloc[10:100, 4].apply(lambda x: [int(x[0]-0.6*x[2]), int(x[1]-0.6*x[3]), x[2], x[3]])\n",
    "dfa.iloc[10:100, 2] = 156\n",
    "dfa.iloc[10:100, 6] = '0_0'\n",
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KGAOz-8mnMJA",
   "metadata": {
    "id": "KGAOz-8mnMJA"
   },
   "outputs": [],
   "source": [
    "class RTSD_dataset_clf_my(Dataset):\n",
    "    def __init__(self, path_img, df, transforms=None):\n",
    "        super().__init__()\n",
    "        self.path_img = path_img\n",
    "        self.transforms = transforms\n",
    "        self.df = df\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        name_img = self.df.loc[index,'file_name']\n",
    "        boxes = self.df.loc[index,'bbox']\n",
    "        bb = [boxes[0], boxes[1], boxes[0] + boxes[2], boxes[1] + boxes[3]]\n",
    "        imgs = Image.open(os.path.join(self.path_img, name_img))\n",
    "        imgs = imgs.crop(bb)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            imgs = self.transforms(imgs)\n",
    "\n",
    "        imgs = imgs / 255\n",
    "\n",
    "        targets = torch.tensor(self.df.loc[index, 'category_id'])\n",
    "\n",
    "\n",
    "#         return imgs, targets\n",
    "        return {\n",
    "                'images': imgs,\n",
    "                'targets': targets}\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S5flDKppnML8",
   "metadata": {
    "id": "S5flDKppnML8"
   },
   "outputs": [],
   "source": [
    "def get_transform1():\n",
    "#     custom_transforms = []\n",
    "#     custom_transforms.append(transforms.ToTensor())\n",
    "    return transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S49pimUtnMOk",
   "metadata": {
    "id": "S49pimUtnMOk"
   },
   "outputs": [],
   "source": [
    "def get_transform_a1():\n",
    "             return transforms.Compose([transforms.Resize((224,224)),\n",
    "                                       transforms.RandomChoice([transforms.Compose([transforms.RandomPerspective(distortion_scale=0.4,p=0.9),\n",
    "                                                                                    transforms.ToTensor()\n",
    "                                                                                    ]),\n",
    "                                                                transforms.Compose([transforms.ColorJitter(brightness=(0.4), contrast=(0.3), saturation=(0.3)),\n",
    "                                                                                    transforms.ToTensor()\n",
    "                                                                                    ]),\n",
    "                                                                transforms.Compose([transforms.RandomResizedCrop((224,224), scale=(0.85, 1)), # Случайная обрезка изображения в диапахоне 85 - 100% и resize в исходный размер\n",
    "                                                                                    transforms.ToTensor()\n",
    "                                                                                    ])        \n",
    "                                                                ])\n",
    "                                       ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GejaC2wonMRc",
   "metadata": {
    "id": "GejaC2wonMRc"
   },
   "outputs": [],
   "source": [
    "df222 = RTSD_dataset_clf_my('./', dfa, transforms=get_transform_a1())\n",
    "df122 = RTSD_dataset_clf_my('./', df_val_anno, transforms=get_transform1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "od8iTqWLnMXs",
   "metadata": {
    "id": "od8iTqWLnMXs"
   },
   "outputs": [],
   "source": [
    "data_loader_train22 = torch.utils.data.DataLoader(df222,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, drop_last = True)\n",
    "\n",
    "data_loader_val22 = torch.utils.data.DataLoader(df122,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xZJhatEJ8JAh",
   "metadata": {
    "id": "xZJhatEJ8JAh"
   },
   "outputs": [],
   "source": [
    "def build_model_clf(classes):\n",
    "    model = mobilenet_v3_large(weights='MobileNet_V3_Large_Weights.IMAGENET1K_V2')\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.classifier[3] = nn.Linear(in_features=model.classifier[3].in_features, out_features=classes)\n",
    "#     model.classifier[3] = nn.Linear(in_features=1280, out_features=classes)\n",
    "    for param in  model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8p-jFxqInMjA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8p-jFxqInMjA",
    "outputId": "86f381d1-6165-4205-b4d5-04a27408624c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-5c1a4163.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-5c1a4163.pth\n",
      "100%|██████████| 21.1M/21.1M [00:00<00:00, 93.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "#model_clf_11 \n",
    "model = build_model_clf(classes1).to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37wQK_PmnMmZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37wQK_PmnMmZ",
    "outputId": "da662d79-12bb-4fd0-f95a-3d1314e46cba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 5.026838302612305\n",
      "epoch: 0, step: 200, loss: 1.0423680543899536\n",
      "epoch: 0, step: 400, loss: 1.0384654998779297\n",
      "epoch: 0, step: 600, loss: 0.7420834898948669\n",
      "epoch: 0, step: 800, loss: 0.7455179691314697\n",
      "Эпоха train: 0 Итераций: 953 train loss: 1.0330450365638433 train acc: 73.75382230972228\n",
      "epoch: 0, step: 0, loss: 0.8517508506774902\n",
      "Потрачено 31.0 минут на 0 эпоху\n",
      "Эпоха val: 0 Итераций: 87 val loss: 0.6932352997105697 val acc: 80.90457929167607\n",
      "epoch: 1, step: 0, loss: 0.6320469379425049\n",
      "epoch: 1, step: 200, loss: 0.6871518492698669\n",
      "epoch: 1, step: 400, loss: 0.5698065757751465\n",
      "epoch: 1, step: 600, loss: 0.8065493702888489\n",
      "epoch: 1, step: 800, loss: 0.9976919293403625\n",
      "Эпоха train: 1 Итераций: 953 train loss: 0.6446597889175195 train acc: 81.70317932392243\n",
      "epoch: 1, step: 0, loss: 0.5056474804878235\n",
      "Потрачено 31.1 минут на 1 эпоху\n",
      "Эпоха val: 1 Итераций: 87 val loss: 0.45386368875530947 val acc: 86.5666591473043\n",
      "epoch: 2, step: 0, loss: 0.5747249126434326\n",
      "epoch: 2, step: 200, loss: 0.49453532695770264\n",
      "epoch: 2, step: 400, loss: 0.5317594408988953\n",
      "epoch: 2, step: 600, loss: 0.5491136312484741\n",
      "epoch: 2, step: 800, loss: 0.5798390507698059\n",
      "Эпоха train: 2 Итераций: 953 train loss: 0.5573109792699844 train acc: 83.87508901269216\n",
      "epoch: 2, step: 0, loss: 0.5103939175605774\n",
      "Потрачено 31.0 минут на 2 эпоху\n",
      "Эпоха val: 2 Итераций: 87 val loss: 0.3902326710950369 val acc: 87.88630724114596\n",
      "epoch: 3, step: 0, loss: 0.49553194642066956\n",
      "epoch: 3, step: 200, loss: 0.6212528944015503\n",
      "epoch: 3, step: 400, loss: 0.6198882460594177\n",
      "epoch: 3, step: 600, loss: 0.8181962370872498\n",
      "epoch: 3, step: 800, loss: 0.4939228892326355\n",
      "Эпоха train: 3 Итераций: 953 train loss: 0.5154691516633799 train acc: 84.99350730951284\n",
      "epoch: 3, step: 0, loss: 0.48638781905174255\n",
      "Потрачено 31.0 минут на 3 эпоху\n",
      "Эпоха val: 3 Итераций: 87 val loss: 0.3784801074485669 val acc: 88.43898037446425\n",
      "epoch: 4, step: 0, loss: 0.4715045094490051\n",
      "epoch: 4, step: 200, loss: 0.5076200366020203\n",
      "epoch: 4, step: 400, loss: 0.7785413861274719\n",
      "epoch: 4, step: 600, loss: 0.29138705134391785\n",
      "epoch: 4, step: 800, loss: 0.5345261096954346\n",
      "Эпоха train: 4 Итераций: 953 train loss: 0.4775207325863063 train acc: 86.06061240732208\n",
      "epoch: 4, step: 0, loss: 0.4520435333251953\n",
      "Потрачено 31.0 минут на 4 эпоху\n",
      "Эпоха val: 4 Итераций: 87 val loss: 0.3421561473059928 val acc: 89.69095420708324\n",
      "epoch: 5, step: 0, loss: 0.46837496757507324\n",
      "epoch: 5, step: 200, loss: 0.3672262132167816\n",
      "epoch: 5, step: 400, loss: 0.3106267750263214\n",
      "epoch: 5, step: 600, loss: 0.30857157707214355\n",
      "epoch: 5, step: 800, loss: 0.3588969111442566\n",
      "Эпоха train: 5 Итераций: 953 train loss: 0.45078359758491154 train acc: 86.64809617559585\n",
      "epoch: 5, step: 0, loss: 0.47283023595809937\n",
      "Потрачено 30.9 минут на 5 эпоху\n",
      "Эпоха val: 5 Итераций: 87 val loss: 0.32608014293785753 val acc: 89.7811865553801\n",
      "epoch: 6, step: 0, loss: 0.5289099216461182\n",
      "epoch: 6, step: 200, loss: 0.3697901964187622\n",
      "epoch: 6, step: 400, loss: 0.535660445690155\n",
      "epoch: 6, step: 600, loss: 0.43240684270858765\n",
      "epoch: 6, step: 800, loss: 0.4839012026786804\n",
      "Эпоха train: 6 Итераций: 953 train loss: 0.440479735137283 train acc: 86.99053323838646\n",
      "epoch: 6, step: 0, loss: 0.4096818268299103\n",
      "Потрачено 31.0 минут на 6 эпоху\n",
      "Эпоха val: 6 Итераций: 87 val loss: 0.3050029598947229 val acc: 90.40153394992105\n",
      "epoch: 7, step: 0, loss: 0.35948628187179565\n",
      "epoch: 7, step: 200, loss: 0.44296491146087646\n",
      "epoch: 7, step: 400, loss: 0.3879137933254242\n",
      "epoch: 7, step: 600, loss: 0.43581536412239075\n",
      "epoch: 7, step: 800, loss: 0.2867746651172638\n",
      "Эпоха train: 7 Итераций: 953 train loss: 0.4120875385835188 train acc: 87.69425711054329\n",
      "epoch: 7, step: 0, loss: 0.3993096947669983\n",
      "Потрачено 31.0 минут на 7 эпоху\n",
      "Эпоха val: 7 Итераций: 87 val loss: 0.30243161911594457 val acc: 90.71734716896007\n",
      "epoch: 8, step: 0, loss: 0.31823623180389404\n",
      "epoch: 8, step: 200, loss: 0.5506613254547119\n",
      "epoch: 8, step: 400, loss: 0.27020370960235596\n",
      "epoch: 8, step: 600, loss: 0.3878266513347626\n",
      "epoch: 8, step: 800, loss: 0.4033459722995758\n",
      "Эпоха train: 8 Итераций: 953 train loss: 0.40924487590539615 train acc: 87.72357893854984\n",
      "epoch: 8, step: 0, loss: 0.4168187379837036\n",
      "Потрачено 30.9 минут на 8 эпоху\n",
      "Эпоха val: 8 Итераций: 87 val loss: 0.3110655438283394 val acc: 90.22106925332731\n",
      "epoch: 9, step: 0, loss: 0.41321128606796265\n",
      "epoch: 9, step: 200, loss: 0.3183400630950928\n",
      "epoch: 9, step: 400, loss: 0.46161162853240967\n",
      "epoch: 9, step: 600, loss: 0.2782164514064789\n",
      "epoch: 9, step: 800, loss: 0.45936158299446106\n",
      "Эпоха train: 9 Итераций: 953 train loss: 0.39046261723056547 train acc: 88.3330540778285\n",
      "epoch: 9, step: 0, loss: 0.42147961258888245\n",
      "Потрачено 30.9 минут на 9 эпоху\n",
      "Эпоха val: 9 Итераций: 87 val loss: 0.2978913427769453 val acc: 91.33769456350102\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []   \n",
    "\n",
    "y_true_t, y_true_v = [], []\n",
    "y_pred_t, y_pred_v = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    train_class_correct = list(0. for i in range(classes1))\n",
    "    train_class_total = list(0. for i in range(classes1))\n",
    "    for i, batch in enumerate(data_loader_train22):\n",
    "        optimizer.zero_grad()\n",
    "        image, targets = batch['images'].to(device), batch['targets'].to(device)\n",
    "#         image = list(img.to(device) for img in image)\n",
    "#         targets = [t.to(device) for t in targets]\n",
    "        # Forward pass.\n",
    "        outputs = model(image)\n",
    "        # Calculate the loss.\n",
    "        loss = loss_func(outputs, targets)\n",
    "        train_running_loss += loss.item()\n",
    "        # Calculate the accuracy.\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_running_correct += (preds == targets).sum().item()\n",
    "        correct  = (preds == targets).squeeze()\n",
    "        for a in range(len(preds)):\n",
    "                label = targets[a]\n",
    "                # print(label)\n",
    "                train_class_correct[label] += correct[a].item()\n",
    "                train_class_total[label] += 1\n",
    "        # Backpropagation.\n",
    "        loss.backward()\n",
    "        # Update the weights.\n",
    "        optimizer.step()     \n",
    "        \n",
    "        y_true_t.extend([int(item) for item in targets])\n",
    "        y_pred_t.extend([int(item) for item in preds])\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "    \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    train_epoch_loss = train_running_loss / i\n",
    "    train_epoch_acc = 100. * (train_running_correct / len(df222))\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_epoch_accuracy = metrics.accuracy_score(y_true_t, y_pred_t)\n",
    "    train_epoch_f1_micro = metrics.f1_score(y_true_t, y_pred_t, average=\"micro\")\n",
    "    train_epoch_f1_macro =  metrics.f1_score(y_true_t, y_pred_t, average=\"macro\")\n",
    "    train_epoch_f1_weighted = metrics.f1_score(y_true_t, y_pred_t, average=\"weighted\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Эпоха train:', epoch,'Итераций:', i, 'train loss:', train_epoch_loss, 'train acc:', train_epoch_acc)\n",
    "    \n",
    "    model.eval()\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    # We need two lists to keep track of class-wise accuracy.\n",
    "    class_correct = list(0. for i in range(classes1))\n",
    "    class_total = list(0. for i in range(classes1))\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader_val22):\n",
    "#             image, targets = batch\n",
    "            image, targets = batch['images'].to(device), batch['targets'].to(device)\n",
    "#             image = list(img.to(device) for img in image)\n",
    "#             targets = [t.to(device) for t in targets]\n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            # Calculate the loss.\n",
    "            loss = loss_func(outputs, targets)\n",
    "            valid_running_loss += loss.item()\n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            valid_running_correct += (preds == targets).sum().item()\n",
    "            # Calculate the accuracy for each class.\n",
    "            correct  = (preds == targets).squeeze()\n",
    "            for j in range(len(preds)):\n",
    "                label = targets[j]\n",
    "                # print(label)\n",
    "                class_correct[label] += correct[j].item()\n",
    "                class_total[label] += 1\n",
    "        \n",
    "            if i % 200 == 0:\n",
    "                 print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "                    \n",
    "            y_true_v.extend([int(item) for item in targets])\n",
    "            y_pred_v.extend([int(item) for item in preds])\n",
    "        \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    valid_epoch_loss = valid_running_loss / i\n",
    "    valid_epoch_acc = 100. * (valid_running_correct / len(df122))\n",
    "    \n",
    "    valid_epoch_accuracy = metrics.accuracy_score(y_true_v, y_pred_v)\n",
    "    valid_epoch_f1_micro = metrics.f1_score(y_true_v, y_pred_v, average=\"micro\")\n",
    "    valid_epoch_f1_macro =  metrics.f1_score(y_true_v, y_pred_v, average=\"macro\")\n",
    "    valid_epoch_f1_weighted = metrics.f1_score(y_true_v, y_pred_v, average=\"weighted\")\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Потрачено {round((end - start) / 60, 1)} минут на {epoch} эпоху\")\n",
    "    print('Эпоха val:', epoch, 'Итераций:', i, 'val loss:', valid_epoch_loss, 'val acc:', valid_epoch_acc)\n",
    "    \n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    train_acc.append(train_epoch_acc)\n",
    "    valid_acc.append(valid_epoch_acc)\n",
    "    \n",
    "    torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'accuracy_train': train_epoch_accuracy,\n",
    "                    'f1_micro_trainl': train_epoch_f1_micro,\n",
    "                    'f1_macro_train': train_epoch_f1_macro,\n",
    "                    'f1_weighted_train': train_epoch_f1_weighted,\n",
    "                    'loss_train': train_loss,\n",
    "                    'acc_train': train_acc,\n",
    "                    'accuracy_val': valid_epoch_accuracy,\n",
    "                    'f1_micro_val': valid_epoch_f1_micro,\n",
    "                    'f1_macro_val': valid_epoch_f1_macro,\n",
    "                    'f1_weighted_val': valid_epoch_f1_weighted,\n",
    "                    'loss_val': valid_loss,\n",
    "                    'acc_val': valid_acc\n",
    "                    }, f'./chkpt_modelv3a_clf_{epoch}.pth')\n",
    "    \n",
    "    torch.save(model.state_dict(), f'./chkpt_mv3a_clf_{epoch}.pth')\n",
    "    !cp chkpt_mv3a_clf_{epoch}.pth /content/drive/MyDrive\n",
    "    !cp chkpt_modelv3a_clf_{epoch}.pth /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310ea60",
   "metadata": {
    "id": "sx0dzY22SnGX"
   },
   "source": [
    "### DETECTOR fasterrcnn_resnet50_fpn_v2 + Аугментация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wyyjBEiJSnJ9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "wyyjBEiJSnJ9",
    "outputId": "0aa34605-57c6-4740-a7f5-eb70a240c85a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 10\n",
    "classes = 2\n",
    "classes1 = 157\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_rrTvncASnMz",
   "metadata": {
    "id": "_rrTvncASnMz"
   },
   "outputs": [],
   "source": [
    "# Приведем все знаки к одному классу\n",
    "df_train_anno_1 = df_train_anno.copy()\n",
    "df_train_anno_1['category_id'] = 1\n",
    "\n",
    "df_val_anno_1 = df_val_anno.copy()\n",
    "df_val_anno_1['category_id'] = 1\n",
    "\n",
    "df_train_anno_reduced_1 = df_train_anno_reduced.copy()\n",
    "df_train_anno_reduced_1['category_id'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u79RedaKSnP1",
   "metadata": {
    "id": "u79RedaKSnP1"
   },
   "outputs": [],
   "source": [
    "class RTSD_dataset_my_a(Dataset):\n",
    "    def __init__(self, path_img, df, transforms=None):\n",
    "        super().__init__()\n",
    "        self.path_img = path_img\n",
    "        self.transforms = transforms\n",
    "        self.df = df\n",
    "        self.df = self.df.groupby('file_name', as_index=False).agg(list)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        name_img = self.df.loc[index,'file_name']\n",
    "        img = Image.open(os.path.join(self.path_img, name_img))\n",
    "        img = np.array(img)\n",
    "      \n",
    "        boxes = []\n",
    "        for b in self.df.loc[index,'bbox']:\n",
    "            bb = [b[0], b[1], b[0] + b[2], b[1] + b[3]]\n",
    "            boxes.append(bb)   \n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.Tensor(self.df.loc[index, 'category_id']).to(torch.int64)\n",
    "        img_id = torch.tensor([self.df.loc[index, 'image_id'][0]])\n",
    "        \n",
    "        areas = []\n",
    "        for i in self.df.loc[index,'area']:\n",
    "            areas.append(self.df.loc[index,'area'])\n",
    "        areas = torch.as_tensor(areas[0], dtype=torch.float32)\n",
    "       \n",
    "        iscrowd = torch.zeros(len(self.df.loc[index,'iscrowd'])).to(torch.int64)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "#         if self.transforms is not None:\n",
    "#             img = self.transforms(img)\n",
    "\n",
    "        if self.transforms:\n",
    "                    sample = self.transforms(image = img,\n",
    "                                             bboxes =  my_annotation['boxes'],\n",
    "                                             labels = labels)\n",
    "                    img = sample['image']\n",
    "                    my_annotation['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "        \n",
    "        img = img / 255\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_YgwEUE-SnTE",
   "metadata": {
    "id": "_YgwEUE-SnTE"
   },
   "outputs": [],
   "source": [
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FEayRIeOSnWS",
   "metadata": {
    "id": "FEayRIeOSnWS"
   },
   "outputs": [],
   "source": [
    "# define the training tranforms\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.MotionBlur(blur_limit=3, p=0.2),\n",
    "        A.Blur(blur_limit=3, p=0.1),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2, p=0.5\n",
    "        ),\n",
    "        A.ColorJitter(p=0.5),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc',\n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "# define the validation transforms\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc', \n",
    "        'label_fields': ['labels']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FlB0kSwzSnZW",
   "metadata": {
    "id": "FlB0kSwzSnZW"
   },
   "outputs": [],
   "source": [
    "df2 = RTSD_dataset_my_a('./', df_train_anno_1,transforms=get_train_transform())\n",
    "df22 = RTSD_dataset_my_a('./', df_val_anno_1,transforms=get_valid_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MLp5e1EoSncS",
   "metadata": {
    "id": "MLp5e1EoSncS"
   },
   "outputs": [],
   "source": [
    "data_loader_train2 = torch.utils.data.DataLoader(df2,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn,\n",
    "                                          drop_last = True)\n",
    "\n",
    "data_loader_val2 = torch.utils.data.DataLoader(df22,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn,\n",
    "                                          drop_last = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r6AC5zdHSnko",
   "metadata": {
    "id": "r6AC5zdHSnko"
   },
   "outputs": [],
   "source": [
    "def build_model(n_model, classes):\n",
    "    if 'resnet50_fpn_v2' in n_model:\n",
    "        model =torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights='FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1')\n",
    "    elif 'resnet50_fpn' in n_model:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"FasterRCNN_ResNet50_FPN_Weights.COCO_V1\")\n",
    "    else:\n",
    "        print(\"нет такой модели \")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T0P7kc3L6hA3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0P7kc3L6hA3",
    "outputId": "416b78c8-fef8-49d4-8f1c-10cf90116cfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n",
      "100%|██████████| 167M/167M [00:01<00:00, 135MB/s]\n"
     ]
    }
   ],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath, map_location=device)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['loss_val']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
    "\n",
    "model = build_model('resnet50_fpn_v2', 2).to(device)\n",
    "params1 = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params1, lr=0.001)\n",
    "\n",
    "model1, optimizer1, start_epoch, loss1_val = load_ckp('./chkpt_model2_d_6.pth', model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272F9LxqSnna",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "272F9LxqSnna",
    "outputId": "1a0561dc-dfc8-43df-eed1-00918a5a0c08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n",
      "100%|██████████| 167M/167M [00:01<00:00, 147MB/s]\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model('resnet50_fpn_v2', classes).to(device)\n",
    "params1 = [p for p in model1.parameters() if p.requires_grad]\n",
    "optimizer1 = torch.optim.Adam(params1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LuZ-DaiOSnqW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LuZ-DaiOSnqW",
    "outputId": "946b1c30-73b8-4a6c-fe8b-0fda2f2ee203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, step: 0, loss: 0.0753016471862793\n",
      "epoch: 7, step: 200, loss: 0.04973529651761055\n",
      "epoch: 7, step: 400, loss: 0.10200314968824387\n",
      "epoch: 7, step: 600, loss: 0.1114787831902504\n",
      "epoch: 7, step: 800, loss: 0.06412721425294876\n",
      "epoch: 7, step: 1000, loss: 0.06871630251407623\n",
      "epoch: 7, step: 1200, loss: 0.05390813201665878\n",
      "epoch: 7, step: 1400, loss: 0.06693138927221298\n",
      "epoch: 7, step: 1600, loss: 0.07883910834789276\n",
      "epoch: 7, step: 1800, loss: 0.07663627713918686\n",
      "epoch: 7, step: 2000, loss: 0.048805687576532364\n",
      "epoch: 7, step: 2200, loss: 0.03328828141093254\n",
      "epoch: 7, step: 2400, loss: 0.049430664628744125\n",
      "epoch: 7, step: 2600, loss: 0.07334597408771515\n",
      "epoch: 7, step: 2800, loss: 0.07179352641105652\n",
      "epoch: 7, step: 3000, loss: 0.0659819170832634\n",
      "epoch: 7, step: 3200, loss: 0.05928252637386322\n",
      "epoch: 7, step: 3400, loss: 0.08463607728481293\n",
      "epoch: 7, step: 3600, loss: 0.06910812109708786\n",
      "epoch: 7, step: 3800, loss: 0.06959029287099838\n",
      "epoch: 7, step: 4000, loss: 0.10676819086074829\n",
      "epoch: 7, step: 4200, loss: 0.06222840026021004\n",
      "epoch: 7, step: 4400, loss: 0.04552116245031357\n",
      "epoch: 7, step: 4600, loss: 0.05162341892719269\n",
      "epoch: 7, step: 4800, loss: 0.054194189608097076\n",
      "epoch: 7, step: 5000, loss: 0.11007896810770035\n",
      "epoch: 7, step: 5200, loss: 0.06267526000738144\n",
      "epoch: 7, step: 5400, loss: 0.08289482444524765\n",
      "Эпоха train: 7 Итераций: 5417 train loss: 1.386198331580427e-05\n",
      "epoch: 7, step: 0, loss: 0.05384579300880432\n",
      "epoch: 7, step: 200, loss: 0.05713829770684242\n",
      "epoch: 7, step: 400, loss: 0.06861481815576553\n",
      "Потрачено 71.3 минут на 7 эпоху\n",
      "Эпоха val: 7 Итераций: 499 val loss: 6.434204194970981e-05\n",
      "epoch: 8, step: 0, loss: 0.0663398876786232\n",
      "epoch: 8, step: 200, loss: 0.048847801983356476\n",
      "epoch: 8, step: 400, loss: 0.11318407207727432\n",
      "epoch: 8, step: 600, loss: 0.06873025000095367\n",
      "epoch: 8, step: 800, loss: 0.03975462168455124\n",
      "epoch: 8, step: 1000, loss: 0.07148495316505432\n",
      "epoch: 8, step: 1200, loss: 0.06678693741559982\n",
      "epoch: 8, step: 1400, loss: 0.055979371070861816\n",
      "epoch: 8, step: 1600, loss: 0.06601747870445251\n",
      "epoch: 8, step: 1800, loss: 0.05321364477276802\n",
      "epoch: 8, step: 2000, loss: 0.06717647612094879\n",
      "epoch: 8, step: 2200, loss: 0.06830818206071854\n",
      "epoch: 8, step: 2400, loss: 0.11854669451713562\n",
      "epoch: 8, step: 2600, loss: 0.06870553642511368\n",
      "epoch: 8, step: 2800, loss: 0.06129032373428345\n",
      "epoch: 8, step: 3000, loss: 0.05795890837907791\n",
      "epoch: 8, step: 3200, loss: 0.07030920684337616\n",
      "epoch: 8, step: 3400, loss: 0.09798874706029892\n",
      "epoch: 8, step: 3600, loss: 0.06936729699373245\n",
      "epoch: 8, step: 3800, loss: 0.07116690278053284\n",
      "epoch: 8, step: 4000, loss: 0.06669867783784866\n",
      "epoch: 8, step: 4200, loss: 0.05205116793513298\n",
      "epoch: 8, step: 4400, loss: 0.0759359821677208\n",
      "epoch: 8, step: 4600, loss: 0.07006356865167618\n",
      "epoch: 8, step: 4800, loss: 0.08959650248289108\n",
      "epoch: 8, step: 5000, loss: 0.05785330384969711\n",
      "epoch: 8, step: 5200, loss: 0.0623784102499485\n",
      "epoch: 8, step: 5400, loss: 0.08267863094806671\n",
      "Эпоха train: 8 Итераций: 5417 train loss: 1.6805703996049538e-05\n",
      "epoch: 8, step: 0, loss: 0.05177241191267967\n",
      "epoch: 8, step: 200, loss: 0.051382821053266525\n",
      "epoch: 8, step: 400, loss: 0.06221902370452881\n",
      "Потрачено 71.3 минут на 8 эпоху\n",
      "Эпоха val: 8 Итераций: 499 val loss: 6.860836534079664e-05\n",
      "epoch: 9, step: 0, loss: 0.10666283965110779\n",
      "epoch: 9, step: 200, loss: 0.09555107355117798\n",
      "epoch: 9, step: 400, loss: 0.058315593749284744\n",
      "epoch: 9, step: 600, loss: 0.057921867817640305\n",
      "epoch: 9, step: 800, loss: 0.05947510525584221\n",
      "epoch: 9, step: 1000, loss: 0.058048319071531296\n",
      "epoch: 9, step: 1200, loss: 0.03663478046655655\n",
      "epoch: 9, step: 1400, loss: 0.05197015404701233\n",
      "epoch: 9, step: 1600, loss: 0.05471883341670036\n",
      "epoch: 9, step: 1800, loss: 0.0723426342010498\n",
      "epoch: 9, step: 2000, loss: 0.08498342335224152\n",
      "epoch: 9, step: 2200, loss: 0.057207249104976654\n",
      "epoch: 9, step: 2400, loss: 0.08164964616298676\n",
      "epoch: 9, step: 2600, loss: 0.09232701361179352\n",
      "epoch: 9, step: 2800, loss: 0.07705821096897125\n",
      "epoch: 9, step: 3000, loss: 0.09740832448005676\n",
      "epoch: 9, step: 3200, loss: 0.07403403520584106\n",
      "epoch: 9, step: 3400, loss: 0.06525851786136627\n",
      "epoch: 9, step: 3600, loss: 0.05253000929951668\n",
      "epoch: 9, step: 3800, loss: 0.059844721108675\n",
      "epoch: 9, step: 4000, loss: 0.04788869619369507\n",
      "epoch: 9, step: 4200, loss: 0.04413750395178795\n",
      "epoch: 9, step: 4400, loss: 0.07160956412553787\n",
      "epoch: 9, step: 4600, loss: 0.04695092514157295\n",
      "epoch: 9, step: 4800, loss: 0.08694423735141754\n",
      "epoch: 9, step: 5000, loss: 0.11016322672367096\n",
      "epoch: 9, step: 5200, loss: 0.11397986114025116\n",
      "epoch: 9, step: 5400, loss: 0.07568686455488205\n",
      "Эпоха train: 9 Итераций: 5417 train loss: 1.484077531780421e-05\n",
      "epoch: 9, step: 0, loss: 0.0620325542986393\n",
      "epoch: 9, step: 200, loss: 0.051492929458618164\n",
      "epoch: 9, step: 400, loss: 0.08252952992916107\n",
      "Потрачено 71.4 минут на 9 эпоху\n",
      "Эпоха val: 9 Итераций: 499 val loss: 7.421035088135866e-05\n"
     ]
    }
   ],
   "source": [
    "loss1_train = []\n",
    "loss1_val = []\n",
    "\n",
    "for epoch in range(7, n_epochs):\n",
    "    start = time.time()\n",
    "    model1.train()\n",
    "    for i, batch in enumerate(data_loader_train2):\n",
    "        optimizer1.zero_grad()\n",
    "        loss_train = 0\n",
    "        imgs, targets = batch\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model1(imgs, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_train += losses.item()\n",
    "        losses.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {losses.item()}')\n",
    "    \n",
    "    loss11_train = (loss_train / i)\n",
    "    loss1_train.append(loss11_train)\n",
    "    \n",
    "    print('Эпоха train:', epoch,'Итераций:', i, 'train loss:', (loss_train / i))\n",
    "    \n",
    "    for i, batch in enumerate(data_loader_val2):\n",
    "        optimizer1.zero_grad()\n",
    "        loss_val = 0\n",
    "        imgs, targets = batch\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.no_grad():\n",
    "              loss_dict = model1(imgs, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_val += losses.item()\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {losses.item()}')\n",
    "    \n",
    "    loss11_val = (loss_val / i)\n",
    "    loss1_val.append(loss11_val)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer1, step_size=3, gamma=0.9)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Потрачено {round((end - start) / 60, 1)} минут на {epoch} эпоху\")\n",
    "    print('Эпоха val:', epoch, 'Итераций:', i, 'val loss:', (loss_val / i))\n",
    "    \n",
    "        \n",
    "    torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model1.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer1.state_dict(),\n",
    "                    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                    'loss_train': loss1_train,\n",
    "                    'loss_val': loss1_val,\n",
    "                    }, f'./chkpt_model2_d_{epoch}.pth')\n",
    "    \n",
    "    torch.save(model1.state_dict(), f'./chkpt_m2_d_{epoch}.pth')\n",
    "    !cp chkpt_model2_d_{epoch}.pth /content/drive/MyDrive\n",
    "    !cp chkpt_m2_d_{epoch}.pth /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dknrbBqHSnwZ",
   "metadata": {
    "id": "dknrbBqHSnwZ"
   },
   "outputs": [],
   "source": [
    "model1 = build_model('resnet50_fpn', 2).to(device)\n",
    "params1 = [p for p in model1.parameters() if p.requires_grad]\n",
    "optimizer1 = torch.optim.Adam(params1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qwfSIbgrX4_l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwfSIbgrX4_l",
    "outputId": "2fa6f8e8-3393-4c8f-a4bd-3f5a25e6a12b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:00<00:00, 333MB/s]\n"
     ]
    }
   ],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath, map_location=device)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['loss_val']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
    "\n",
    "model = build_model('resnet50_fpn', 2).to(device)\n",
    "params1 = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params1, lr=0.001)\n",
    "\n",
    "model1, optimizer1, start_epoch, loss1_val = load_ckp('./chkpt_model1aa_d_0.pth', model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "uA6eTTx0SnzO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uA6eTTx0SnzO",
    "outputId": "799ab36a-20dc-4f58-dcbf-db47b7f04b9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 0, loss: 0.07571166008710861\n",
      "epoch: 1, step: 200, loss: 0.07042182981967926\n",
      "epoch: 1, step: 400, loss: 0.11172984540462494\n",
      "epoch: 1, step: 600, loss: 0.10133524984121323\n",
      "epoch: 1, step: 800, loss: 0.11728022992610931\n",
      "epoch: 1, step: 1000, loss: 0.14196282625198364\n",
      "epoch: 1, step: 1200, loss: 0.08096003532409668\n",
      "epoch: 1, step: 1400, loss: 0.08901054412126541\n",
      "epoch: 1, step: 1600, loss: 0.10958066582679749\n",
      "epoch: 1, step: 1800, loss: 0.133158877491951\n",
      "epoch: 1, step: 2000, loss: 0.16329233348369598\n",
      "epoch: 1, step: 2200, loss: 0.08531792461872101\n",
      "epoch: 1, step: 2400, loss: 0.07471255958080292\n",
      "epoch: 1, step: 2600, loss: 0.08446311205625534\n",
      "epoch: 1, step: 2800, loss: 0.06701342761516571\n",
      "epoch: 1, step: 3000, loss: 0.0834004357457161\n",
      "epoch: 1, step: 3200, loss: 0.1365889459848404\n",
      "epoch: 1, step: 3400, loss: 0.8244222402572632\n",
      "epoch: 1, step: 3600, loss: 0.17560029029846191\n",
      "epoch: 1, step: 3800, loss: 0.5143008232116699\n",
      "epoch: 1, step: 4000, loss: 0.6085484623908997\n",
      "epoch: 1, step: 4200, loss: 0.41568148136138916\n",
      "epoch: 1, step: 4400, loss: 0.4158572554588318\n",
      "epoch: 1, step: 4600, loss: 0.37763774394989014\n",
      "epoch: 1, step: 4800, loss: 0.2545437514781952\n",
      "epoch: 1, step: 5000, loss: 0.2786286771297455\n",
      "epoch: 1, step: 5200, loss: 0.2036682665348053\n",
      "epoch: 1, step: 5400, loss: 0.25122082233428955\n",
      "Эпоха train: 1 Итераций: 5417 train loss: 3.775506413949027e-05\n",
      "epoch: 1, step: 0, loss: 0.24379970133304596\n",
      "epoch: 1, step: 200, loss: 0.23191002011299133\n",
      "epoch: 1, step: 400, loss: 0.19667579233646393\n",
      "Потрачено 61.3 минут на 1 эпоху\n",
      "Эпоха val: 1 Итераций: 499 val loss: 0.00036238005500518247\n",
      "epoch: 2, step: 0, loss: 0.3062953054904938\n",
      "epoch: 2, step: 200, loss: 0.2133682519197464\n",
      "epoch: 2, step: 400, loss: 0.3472175598144531\n",
      "epoch: 2, step: 600, loss: 0.1940757930278778\n",
      "epoch: 2, step: 800, loss: 0.2887909710407257\n",
      "epoch: 2, step: 1000, loss: 0.21142052114009857\n",
      "epoch: 2, step: 1200, loss: 0.20953696966171265\n",
      "epoch: 2, step: 1400, loss: 0.2382374405860901\n",
      "epoch: 2, step: 1600, loss: 0.3125678300857544\n",
      "epoch: 2, step: 1800, loss: 0.18985244631767273\n",
      "epoch: 2, step: 2000, loss: 0.15393762290477753\n",
      "epoch: 2, step: 2200, loss: 0.161021888256073\n",
      "epoch: 2, step: 2400, loss: 0.1472877860069275\n",
      "epoch: 2, step: 2600, loss: 0.26566606760025024\n",
      "epoch: 2, step: 2800, loss: 0.23368793725967407\n",
      "epoch: 2, step: 3000, loss: 0.1351395845413208\n",
      "epoch: 2, step: 3200, loss: 0.11646118015050888\n",
      "epoch: 2, step: 3400, loss: 0.268877774477005\n",
      "epoch: 2, step: 3600, loss: 0.36551421880722046\n",
      "epoch: 2, step: 3800, loss: 0.20530438423156738\n",
      "epoch: 2, step: 4000, loss: 0.3123752474784851\n",
      "epoch: 2, step: 4200, loss: 0.23596034944057465\n",
      "epoch: 2, step: 4400, loss: 0.15383729338645935\n",
      "epoch: 2, step: 4600, loss: 0.1942003220319748\n",
      "epoch: 2, step: 4800, loss: 0.3970899283885956\n",
      "epoch: 2, step: 5000, loss: 0.1604461669921875\n",
      "epoch: 2, step: 5200, loss: 0.22806453704833984\n",
      "epoch: 2, step: 5400, loss: 0.20222413539886475\n",
      "Эпоха train: 2 Итераций: 5417 train loss: 2.3507279114725846e-05\n",
      "epoch: 2, step: 0, loss: 0.20383518934249878\n",
      "epoch: 2, step: 200, loss: 0.18574224412441254\n",
      "epoch: 2, step: 400, loss: 0.13521808385849\n",
      "Потрачено 61.1 минут на 2 эпоху\n",
      "Эпоха val: 2 Итераций: 499 val loss: 0.00021875425725279447\n",
      "epoch: 3, step: 0, loss: 0.18373936414718628\n",
      "epoch: 3, step: 200, loss: 0.19179588556289673\n",
      "epoch: 3, step: 400, loss: 0.18156248331069946\n",
      "epoch: 3, step: 600, loss: 0.2238294929265976\n",
      "epoch: 3, step: 800, loss: 0.19922955334186554\n",
      "epoch: 3, step: 1000, loss: 0.1801469326019287\n",
      "epoch: 3, step: 1200, loss: 0.2844018042087555\n",
      "epoch: 3, step: 1400, loss: 0.33870017528533936\n",
      "epoch: 3, step: 1600, loss: 0.22770991921424866\n",
      "epoch: 3, step: 1800, loss: 0.15564917027950287\n",
      "epoch: 3, step: 2000, loss: 0.22863824665546417\n",
      "epoch: 3, step: 2200, loss: 0.15138153731822968\n",
      "epoch: 3, step: 2400, loss: 0.12766118347644806\n",
      "epoch: 3, step: 2600, loss: 0.14255894720554352\n",
      "epoch: 3, step: 2800, loss: 0.23253753781318665\n",
      "epoch: 3, step: 3000, loss: 0.1719599813222885\n",
      "epoch: 3, step: 3200, loss: 0.3074394464492798\n",
      "epoch: 3, step: 3400, loss: 0.20669932663440704\n",
      "epoch: 3, step: 3600, loss: 0.46211057901382446\n",
      "epoch: 3, step: 3800, loss: 0.2751506567001343\n",
      "epoch: 3, step: 4000, loss: 0.2288253754377365\n",
      "epoch: 3, step: 4200, loss: 0.14721105992794037\n",
      "epoch: 3, step: 4400, loss: 0.11682373285293579\n",
      "epoch: 3, step: 4600, loss: 0.16141779720783234\n",
      "epoch: 3, step: 4800, loss: 0.4062539339065552\n",
      "epoch: 3, step: 5000, loss: 0.14519743621349335\n",
      "epoch: 3, step: 5200, loss: 0.1446925401687622\n",
      "epoch: 3, step: 5400, loss: 0.16903428733348846\n",
      "Эпоха train: 3 Итераций: 5417 train loss: 4.09897439524281e-05\n",
      "epoch: 3, step: 0, loss: 0.2033979743719101\n",
      "epoch: 3, step: 200, loss: 0.1849944293498993\n",
      "epoch: 3, step: 400, loss: 0.13480336964130402\n",
      "Потрачено 61.1 минут на 3 эпоху\n",
      "Эпоха val: 3 Итераций: 499 val loss: 0.00021748186531430017\n",
      "epoch: 4, step: 0, loss: 0.16468693315982819\n",
      "epoch: 4, step: 200, loss: 0.17690925300121307\n",
      "epoch: 4, step: 400, loss: 0.2142961472272873\n",
      "epoch: 4, step: 600, loss: 0.21017900109291077\n",
      "epoch: 4, step: 800, loss: 0.14790703356266022\n",
      "epoch: 4, step: 1000, loss: 0.22874009609222412\n",
      "epoch: 4, step: 1200, loss: 0.3281930685043335\n",
      "epoch: 4, step: 1400, loss: 0.27362188696861267\n",
      "epoch: 4, step: 1600, loss: 0.21268230676651\n",
      "epoch: 4, step: 1800, loss: 0.1702558845281601\n",
      "epoch: 4, step: 2000, loss: 0.1495799422264099\n",
      "epoch: 4, step: 2200, loss: 0.12723298370838165\n",
      "epoch: 4, step: 2400, loss: 0.21036837995052338\n",
      "epoch: 4, step: 2600, loss: 0.2274392545223236\n",
      "epoch: 4, step: 2800, loss: 0.2579737603664398\n",
      "epoch: 4, step: 3000, loss: 0.12405470758676529\n",
      "epoch: 4, step: 3200, loss: 0.13867947459220886\n",
      "epoch: 4, step: 3400, loss: 0.22030465304851532\n",
      "epoch: 4, step: 3600, loss: 0.21655720472335815\n",
      "epoch: 4, step: 3800, loss: 0.11472238600254059\n",
      "epoch: 4, step: 4000, loss: 0.1397104263305664\n",
      "epoch: 4, step: 4200, loss: 0.18339794874191284\n",
      "epoch: 4, step: 4400, loss: 0.19431276619434357\n",
      "epoch: 4, step: 4600, loss: 0.16537687182426453\n",
      "epoch: 4, step: 4800, loss: 0.18758554756641388\n",
      "epoch: 4, step: 5000, loss: 0.15731531381607056\n",
      "epoch: 4, step: 5200, loss: 0.22452549636363983\n",
      "epoch: 4, step: 5400, loss: 0.18176500499248505\n",
      "Эпоха train: 4 Итераций: 5417 train loss: 2.4869881236115264e-05\n",
      "epoch: 4, step: 0, loss: 0.20339526236057281\n",
      "epoch: 4, step: 200, loss: 0.1850803941488266\n",
      "epoch: 4, step: 400, loss: 0.13520017266273499\n",
      "Потрачено 61.2 минут на 4 эпоху\n",
      "Эпоха val: 4 Итераций: 499 val loss: 0.0002190420228637053\n",
      "epoch: 5, step: 0, loss: 0.23018597066402435\n",
      "epoch: 5, step: 200, loss: 0.2497159242630005\n",
      "epoch: 5, step: 400, loss: 0.1610206961631775\n",
      "epoch: 5, step: 600, loss: 0.20877845585346222\n",
      "epoch: 5, step: 800, loss: 0.2907470762729645\n",
      "epoch: 5, step: 1000, loss: 0.12658096849918365\n",
      "epoch: 5, step: 1200, loss: 0.3073946237564087\n",
      "epoch: 5, step: 1400, loss: 0.2588699460029602\n",
      "epoch: 5, step: 1600, loss: 0.3166109621524811\n",
      "epoch: 5, step: 1800, loss: 0.32306045293807983\n",
      "epoch: 5, step: 2000, loss: 0.17747706174850464\n",
      "epoch: 5, step: 2200, loss: 0.3646676242351532\n",
      "epoch: 5, step: 2400, loss: 0.1720649003982544\n",
      "epoch: 5, step: 2600, loss: 0.19509680569171906\n",
      "epoch: 5, step: 2800, loss: 0.21755872666835785\n",
      "epoch: 5, step: 3000, loss: 0.13367919623851776\n",
      "epoch: 5, step: 3200, loss: 0.15673516690731049\n",
      "epoch: 5, step: 3400, loss: 0.1446537971496582\n",
      "epoch: 5, step: 3600, loss: 0.12199816852807999\n",
      "epoch: 5, step: 3800, loss: 0.16773337125778198\n",
      "epoch: 5, step: 4000, loss: 0.29637593030929565\n",
      "epoch: 5, step: 4200, loss: 0.22670654952526093\n",
      "epoch: 5, step: 4400, loss: 0.15662015974521637\n",
      "epoch: 5, step: 4600, loss: 0.15367241203784943\n",
      "epoch: 5, step: 4800, loss: 0.20187364518642426\n",
      "epoch: 5, step: 5000, loss: 0.1745324432849884\n",
      "epoch: 5, step: 5200, loss: 0.18532687425613403\n",
      "epoch: 5, step: 5400, loss: 0.20400413870811462\n",
      "Эпоха train: 5 Итераций: 5417 train loss: 3.457242148112212e-05\n",
      "epoch: 5, step: 0, loss: 0.2035585641860962\n",
      "epoch: 5, step: 200, loss: 0.18457089364528656\n",
      "epoch: 5, step: 400, loss: 0.13488110899925232\n",
      "Потрачено 61.3 минут на 5 эпоху\n",
      "Эпоха val: 5 Итераций: 499 val loss: 0.00021798038231824826\n",
      "epoch: 6, step: 0, loss: 0.20714013278484344\n",
      "epoch: 6, step: 200, loss: 0.16456164419651031\n",
      "epoch: 6, step: 400, loss: 0.20687274634838104\n",
      "epoch: 6, step: 600, loss: 0.19976672530174255\n",
      "epoch: 6, step: 800, loss: 0.2164648473262787\n",
      "epoch: 6, step: 1000, loss: 0.22747689485549927\n",
      "epoch: 6, step: 1200, loss: 0.25165626406669617\n",
      "epoch: 6, step: 1400, loss: 0.20864227414131165\n",
      "epoch: 6, step: 1600, loss: 0.26823684573173523\n",
      "epoch: 6, step: 1800, loss: 0.1950138956308365\n",
      "epoch: 6, step: 2000, loss: 0.2211800366640091\n",
      "epoch: 6, step: 2200, loss: 0.1969624012708664\n",
      "epoch: 6, step: 2400, loss: 0.2964175045490265\n",
      "epoch: 6, step: 2600, loss: 0.212534099817276\n",
      "epoch: 6, step: 2800, loss: 0.2056693136692047\n",
      "epoch: 6, step: 3000, loss: 0.17719921469688416\n",
      "epoch: 6, step: 3200, loss: 0.2545963227748871\n",
      "epoch: 6, step: 3400, loss: 0.23817960917949677\n",
      "epoch: 6, step: 3600, loss: 0.16939498484134674\n",
      "epoch: 6, step: 3800, loss: 0.19693244993686676\n",
      "epoch: 6, step: 4000, loss: 0.1698654294013977\n",
      "epoch: 6, step: 4200, loss: 0.18357978761196136\n",
      "epoch: 6, step: 4400, loss: 0.17982856929302216\n",
      "epoch: 6, step: 4600, loss: 0.16202080249786377\n",
      "epoch: 6, step: 4800, loss: 0.16743646562099457\n",
      "epoch: 6, step: 5000, loss: 0.2411312609910965\n",
      "epoch: 6, step: 5200, loss: 0.14480799436569214\n",
      "epoch: 6, step: 5400, loss: 0.3501428961753845\n",
      "Эпоха train: 6 Итераций: 5417 train loss: 3.9421793541741894e-05\n",
      "epoch: 6, step: 0, loss: 0.20379620790481567\n",
      "epoch: 6, step: 200, loss: 0.18541651964187622\n",
      "epoch: 6, step: 400, loss: 0.13558277487754822\n",
      "Потрачено 61.2 минут на 6 эпоху\n",
      "Эпоха val: 6 Итераций: 499 val loss: 0.00021838823814908107\n",
      "epoch: 7, step: 0, loss: 0.15502510964870453\n",
      "epoch: 7, step: 200, loss: 0.1478673815727234\n",
      "epoch: 7, step: 400, loss: 0.2834782302379608\n",
      "epoch: 7, step: 600, loss: 0.20695941150188446\n",
      "epoch: 7, step: 800, loss: 0.20016241073608398\n",
      "epoch: 7, step: 1000, loss: 0.3379790484905243\n",
      "epoch: 7, step: 1200, loss: 0.2945423424243927\n",
      "epoch: 7, step: 1400, loss: 0.33372315764427185\n",
      "epoch: 7, step: 1600, loss: 0.4814990162849426\n",
      "epoch: 7, step: 1800, loss: 0.2505650818347931\n",
      "epoch: 7, step: 2000, loss: 0.13728176057338715\n",
      "epoch: 7, step: 2200, loss: 0.18520469963550568\n",
      "epoch: 7, step: 2400, loss: 0.2503691613674164\n",
      "epoch: 7, step: 2600, loss: 0.3087126612663269\n",
      "epoch: 7, step: 2800, loss: 0.15609268844127655\n",
      "epoch: 7, step: 3000, loss: 0.17724864184856415\n",
      "epoch: 7, step: 3200, loss: 0.23407375812530518\n",
      "epoch: 7, step: 3400, loss: 0.14259925484657288\n",
      "epoch: 7, step: 3600, loss: 0.19227048754692078\n",
      "epoch: 7, step: 3800, loss: 0.2520958483219147\n",
      "epoch: 7, step: 4000, loss: 0.15939831733703613\n",
      "epoch: 7, step: 4200, loss: 0.1736038774251938\n",
      "epoch: 7, step: 4400, loss: 0.2543793022632599\n",
      "epoch: 7, step: 4600, loss: 0.14571060240268707\n",
      "epoch: 7, step: 4800, loss: 0.1746414452791214\n",
      "epoch: 7, step: 5000, loss: 0.18824546039104462\n",
      "epoch: 7, step: 5200, loss: 0.3279224634170532\n",
      "epoch: 7, step: 5400, loss: 0.17695707082748413\n",
      "Эпоха train: 7 Итераций: 5417 train loss: 2.7149338024182362e-05\n",
      "epoch: 7, step: 0, loss: 0.20351897180080414\n",
      "epoch: 7, step: 200, loss: 0.1849653571844101\n",
      "epoch: 7, step: 400, loss: 0.13545851409435272\n",
      "Потрачено 61.4 минут на 7 эпоху\n",
      "Эпоха val: 7 Итераций: 499 val loss: 0.00021920659260186022\n",
      "epoch: 8, step: 0, loss: 0.21726498007774353\n",
      "epoch: 8, step: 200, loss: 0.23064562678337097\n",
      "epoch: 8, step: 400, loss: 0.14609305560588837\n",
      "epoch: 8, step: 600, loss: 0.3057098388671875\n",
      "epoch: 8, step: 800, loss: 0.33842477202415466\n",
      "epoch: 8, step: 1000, loss: 0.1350139081478119\n",
      "epoch: 8, step: 1200, loss: 0.15035006403923035\n",
      "epoch: 8, step: 1400, loss: 0.26522552967071533\n",
      "epoch: 8, step: 1600, loss: 0.286233127117157\n",
      "epoch: 8, step: 1800, loss: 0.19931280612945557\n",
      "epoch: 8, step: 2000, loss: 0.186295747756958\n",
      "epoch: 8, step: 2200, loss: 0.228914275765419\n",
      "epoch: 8, step: 2400, loss: 0.2444770634174347\n",
      "epoch: 8, step: 2600, loss: 0.26042360067367554\n",
      "epoch: 8, step: 2800, loss: 0.1891157329082489\n",
      "epoch: 8, step: 3000, loss: 0.3600732088088989\n",
      "epoch: 8, step: 3200, loss: 0.13895095884799957\n",
      "epoch: 8, step: 3400, loss: 0.23494745790958405\n",
      "epoch: 8, step: 3600, loss: 0.2838541269302368\n",
      "epoch: 8, step: 3800, loss: 0.17217731475830078\n",
      "epoch: 8, step: 4000, loss: 0.3267611861228943\n",
      "epoch: 8, step: 4200, loss: 0.21199266612529755\n",
      "epoch: 8, step: 4400, loss: 0.26775652170181274\n",
      "epoch: 8, step: 4600, loss: 0.19107049703598022\n",
      "epoch: 8, step: 4800, loss: 0.14306338131427765\n",
      "epoch: 8, step: 5000, loss: 0.20299269258975983\n",
      "epoch: 8, step: 5200, loss: 0.21265935897827148\n",
      "epoch: 8, step: 5400, loss: 0.19606341421604156\n",
      "Эпоха train: 8 Итераций: 5417 train loss: 2.2575985423660772e-05\n",
      "epoch: 8, step: 0, loss: 0.20345675945281982\n",
      "epoch: 8, step: 200, loss: 0.18540063500404358\n",
      "epoch: 8, step: 400, loss: 0.1345634162425995\n",
      "Потрачено 61.3 минут на 8 эпоху\n",
      "Эпоха val: 8 Итераций: 499 val loss: 0.00021690976822782374\n",
      "epoch: 9, step: 0, loss: 0.11588482558727264\n",
      "epoch: 9, step: 200, loss: 0.1580892950296402\n",
      "epoch: 9, step: 400, loss: 0.25487616658210754\n",
      "epoch: 9, step: 600, loss: 0.1862482875585556\n",
      "epoch: 9, step: 800, loss: 0.25919681787490845\n",
      "epoch: 9, step: 1000, loss: 0.2093016356229782\n",
      "epoch: 9, step: 1200, loss: 0.2582297623157501\n",
      "epoch: 9, step: 1400, loss: 0.19451488554477692\n",
      "epoch: 9, step: 1600, loss: 0.21223627030849457\n",
      "epoch: 9, step: 1800, loss: 0.18454216420650482\n",
      "epoch: 9, step: 2000, loss: 0.12574352324008942\n",
      "epoch: 9, step: 2200, loss: 0.2502794563770294\n",
      "epoch: 9, step: 2400, loss: 0.14489737153053284\n",
      "epoch: 9, step: 2600, loss: 0.220835879445076\n",
      "epoch: 9, step: 2800, loss: 0.38139456510543823\n",
      "epoch: 9, step: 3000, loss: 0.21336078643798828\n",
      "epoch: 9, step: 3200, loss: 0.17000901699066162\n",
      "epoch: 9, step: 3400, loss: 0.22479219734668732\n",
      "epoch: 9, step: 3600, loss: 0.14825546741485596\n",
      "epoch: 9, step: 3800, loss: 0.1491963118314743\n",
      "epoch: 9, step: 4000, loss: 0.15281715989112854\n",
      "epoch: 9, step: 4200, loss: 0.19893792271614075\n",
      "epoch: 9, step: 4400, loss: 0.18279092013835907\n",
      "epoch: 9, step: 4600, loss: 0.22203229367733002\n",
      "epoch: 9, step: 4800, loss: 0.2412996143102646\n",
      "epoch: 9, step: 5000, loss: 0.2377985566854477\n",
      "epoch: 9, step: 5200, loss: 0.1546398103237152\n",
      "epoch: 9, step: 5400, loss: 0.1861552745103836\n",
      "Эпоха train: 9 Итераций: 5417 train loss: 3.1180158518811795e-05\n",
      "epoch: 9, step: 0, loss: 0.20361323654651642\n",
      "epoch: 9, step: 200, loss: 0.1859314888715744\n",
      "epoch: 9, step: 400, loss: 0.13547392189502716\n",
      "Потрачено 61.1 минут на 9 эпоху\n",
      "Эпоха val: 9 Итераций: 499 val loss: 0.00021965085026735294\n"
     ]
    }
   ],
   "source": [
    "loss1_train = []\n",
    "loss1_val = []\n",
    "\n",
    "for epoch in range(1, n_epochs):\n",
    "    start = time.time()\n",
    "    model1.train()\n",
    "    for i, batch in enumerate(data_loader_train2):\n",
    "        optimizer1.zero_grad()\n",
    "        loss_train = 0\n",
    "        imgs, targets = batch\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model1(imgs, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_train += losses.item()\n",
    "        losses.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {losses.item()}')\n",
    "    \n",
    "    loss11_train = (loss_train / i)\n",
    "    loss1_train.append(loss11_train)\n",
    "    \n",
    "    print('Эпоха train:', epoch,'Итераций:', i, 'train loss:', (loss_train / i))\n",
    "    \n",
    "    for i, batch in enumerate(data_loader_val2):\n",
    "        optimizer1.zero_grad()\n",
    "        loss_val = 0\n",
    "        imgs, targets = batch\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.no_grad():\n",
    "              loss_dict = model1(imgs, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_val += losses.item()\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {losses.item()}')\n",
    "    \n",
    "    loss11_val = (loss_val / i)\n",
    "    loss1_val.append(loss11_val)\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer1, step_size=3, gamma=0.9)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Потрачено {round((end - start) / 60, 1)} минут на {epoch} эпоху\")\n",
    "    print('Эпоха val:', epoch, 'Итераций:', i, 'val loss:', (loss_val / i))\n",
    "    \n",
    "        \n",
    "    torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model1.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer1.state_dict(),\n",
    "                    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                    'loss_train': loss1_train,\n",
    "                    'loss_val': loss1_val,\n",
    "                    }, f'./chkpt_model1aa_d_{epoch}.pth')\n",
    "    \n",
    "    torch.save(model1.state_dict(), f'./chkpt_m1aa_d_{epoch}.pth')\n",
    "    !cp chkpt_model1aa_d_{epoch}.pth /content/drive/MyDrive\n",
    "    !cp chkpt_m1aa_d_{epoch}.pth /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730317f",
   "metadata": {},
   "source": [
    "### CLF_mobilenet_v3_large на маленькой выборке train_anno_reduced увеличенной в 20 раз + Аугментация данных  на большой выборке train_anno + Аугментация данных + class не знак"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5GcmF08S-K5B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "5GcmF08S-K5B",
    "outputId": "a1e68b8e-cb4e-4e60-d7a7-baa2c8567a62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "classes = 2\n",
    "classes1 = 157\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "N2NJNRgR-K8H",
   "metadata": {
    "id": "N2NJNRgR-K8H"
   },
   "outputs": [],
   "source": [
    "def dataset(path, name_f):\n",
    "  \n",
    "  with open(os.path.join(path, name_f), 'r') as f:\n",
    "    anno = json.load(f)\n",
    "\n",
    "    obj1 = anno.get('images')\n",
    "    df1 = pd.json_normalize(obj1)\n",
    "    obj2 = anno.get('annotations')\n",
    "    df2 = pd.json_normalize(obj2)\n",
    "    obj3 = anno.get('categories')\n",
    "    df3 = pd.json_normalize(obj3)\n",
    "    t = df2.merge(df3.set_index('id'), left_on='category_id',right_index=True)\n",
    "    df= t.merge(df1.set_index('id'), left_on='image_id',right_index=True)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "NrdddZJb-K_P",
   "metadata": {
    "id": "NrdddZJb-K_P"
   },
   "outputs": [],
   "source": [
    "df_train_anno = dataset('.', 'train_anno.json')\n",
    "df_val_anno = dataset('.', 'val_anno.json')\n",
    "df_train_anno_reduced = dataset('.', 'train_anno_reduced.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bVAWfcp2-LB-",
   "metadata": {
    "id": "bVAWfcp2-LB-"
   },
   "outputs": [],
   "source": [
    "class RTSD_dataset_clf_my(Dataset):\n",
    "    def __init__(self, path_img, df, transforms=None):\n",
    "        super().__init__()\n",
    "        self.path_img = path_img\n",
    "        self.transforms = transforms\n",
    "        self.df = df\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        name_img = self.df.loc[index,'file_name']\n",
    "        boxes = self.df.loc[index,'bbox']\n",
    "        bb = [boxes[0], boxes[1], boxes[0] + boxes[2], boxes[1] + boxes[3]]\n",
    "        imgs = Image.open(os.path.join(self.path_img, name_img))\n",
    "        imgs = imgs.crop(bb)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            imgs = self.transforms(imgs)\n",
    "\n",
    "        imgs = imgs / 255\n",
    "\n",
    "        targets = torch.tensor(self.df.loc[index, 'category_id'])\n",
    "\n",
    "\n",
    "#         return imgs, targets\n",
    "        return {\n",
    "                'images': imgs,\n",
    "                'targets': targets}\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "yZABOka--LFW",
   "metadata": {
    "id": "yZABOka--LFW"
   },
   "outputs": [],
   "source": [
    "def get_transform1():\n",
    "#     custom_transforms = []\n",
    "#     custom_transforms.append(transforms.ToTensor())\n",
    "    return transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dofN8Whi-LH7",
   "metadata": {
    "id": "dofN8Whi-LH7"
   },
   "outputs": [],
   "source": [
    "def get_transform_a1():\n",
    "             return transforms.Compose([transforms.Resize((224,224)),\n",
    "                                       transforms.RandomChoice([transforms.Compose([transforms.RandomPerspective(distortion_scale=0.4,p=0.9),\n",
    "                                                                                    transforms.ToTensor()\n",
    "                                                                                    ]),\n",
    "                                                                transforms.Compose([transforms.ColorJitter(brightness=(0.4), contrast=(0.3), saturation=(0.3)),\n",
    "                                                                                    transforms.ToTensor()\n",
    "                                                                                    ]),\n",
    "                                                                transforms.Compose([transforms.RandomResizedCrop((224,224), scale=(0.85, 1)), # Случайная обрезка изображения в диапахоне 85 - 100% и resize в исходный размер\n",
    "                                                                                    transforms.ToTensor()\n",
    "                                                                                    ])        \n",
    "                                                                ])\n",
    "                                       ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "gurxj-2Q-LOK",
   "metadata": {
    "id": "gurxj-2Q-LOK"
   },
   "outputs": [],
   "source": [
    "dfb = df_train_anno_reduced.copy().reset_index()\n",
    "dfb = dfb.drop(['index'],axis=1)\n",
    "sorted = dfb.sort_values(['category_id'])\n",
    "first = sorted.groupby('category_id').first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "wVEhfIcv-LR6",
   "metadata": {
    "id": "wVEhfIcv-LR6"
   },
   "outputs": [],
   "source": [
    "dfa = first.copy().reset_index()\n",
    "dfa = dfa.drop(['index'],axis=1)\n",
    "dfa['bbox'] = dfa['bbox'].apply(lambda x: [int(x[0]-0.6*x[2]), int(x[1]-0.6*x[3]), x[2], x[3]])\n",
    "dfa['category_id'] = 156\n",
    "dfa['name'] = '0_0'\n",
    "dfaa = pd.DataFrame(np.repeat(dfa.values, 2, axis=0), columns=dfa.columns)\n",
    "new_df = pd.DataFrame(np.repeat(df_train_anno_reduced.values, 21, axis=0), columns=df_train_anno_reduced.columns)\n",
    "frames = [new_df, dfaa]\n",
    "result = pd.concat(frames).reset_index()\n",
    "result = result.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "BHSOgD6e-LVM",
   "metadata": {
    "id": "BHSOgD6e-LVM"
   },
   "outputs": [],
   "source": [
    "dfa = first.copy().reset_index()\n",
    "dfa = dfa.drop(['index'],axis=1)\n",
    "dfa['bbox'] = dfa['bbox'].apply(lambda x: [int(x[0]-0.6*x[2]), int(x[1]-0.6*x[3]), x[2], x[3]])\n",
    "dfa['category_id'] = 156\n",
    "dfa['name'] = '0_0'\n",
    "dfaa = pd.DataFrame(np.repeat(dfa.values, 2, axis=0), columns=dfa.columns)\n",
    "new_df = pd.DataFrame(np.repeat(df_train_anno_reduced.values, 21, axis=0), columns=df_train_anno_reduced.columns)\n",
    "frames = [new_df, dfaa]\n",
    "result = pd.concat(frames).reset_index()\n",
    "result = result.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "MN3wVNCk-LYY",
   "metadata": {
    "id": "MN3wVNCk-LYY"
   },
   "outputs": [],
   "source": [
    "df12 = RTSD_dataset_clf_my('./', result, transforms=get_transform_a1())\n",
    "\n",
    "df120 = RTSD_dataset_clf_my('./', df_train_anno_reduced, transforms=get_transform1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "CpuALdwG-LbM",
   "metadata": {
    "id": "CpuALdwG-LbM"
   },
   "outputs": [],
   "source": [
    "data_loader_train12 = torch.utils.data.DataLoader(df12,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, drop_last = True)\n",
    "\n",
    "data_loader_val12 = torch.utils.data.DataLoader(df120,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "mfauMjZv-Lef",
   "metadata": {
    "id": "mfauMjZv-Lef"
   },
   "outputs": [],
   "source": [
    "def build_model_clf(classes):\n",
    "    model = mobilenet_v3_large(weights='MobileNet_V3_Large_Weights.IMAGENET1K_V2')\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.classifier[3] = nn.Linear(in_features=model.classifier[3].in_features, out_features=classes)\n",
    "#     model.classifier[3] = nn.Linear(in_features=1280, out_features=classes)\n",
    "    for param in  model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3obY7Exs-LhZ",
   "metadata": {
    "id": "3obY7Exs-LhZ"
   },
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath, map_location=device)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['loss_val']\n",
    "    train_loss_min = checkpoint['loss_train']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min, train_loss_min\n",
    "\n",
    "\n",
    "model1 = build_model_clf(157).to(device)\n",
    "params1 = [p for p in model1.parameters() if p.requires_grad]\n",
    "optimizer1 = torch.optim.Adam(params1, lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "model, optimizer, start_epoch, valid_loss, train_loss = load_ckp('./chkpt_modelrv3_clf_32.pth', model1, optimizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sxGy_opC-LkO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxGy_opC-LkO",
    "outputId": "58061529-abde-4a41-da8e-5c804200256a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33, step: 0, loss: 0.006391296163201332\n",
      "epoch: 33, step: 200, loss: 0.004785778932273388\n",
      "epoch: 33, step: 400, loss: 0.03172260895371437\n",
      "Эпоха train: 33 Итераций: 490 train loss: 0.055775482388811984 train acc: 98.3535158237509\n",
      "epoch: 33, step: 0, loss: 2.4102059796859976e-06\n",
      "Потрачено 14.5 минут на 33 эпоху\n",
      "Эпоха val: 33 Итераций: 22 val loss: 5.125233752653361e-06 val acc: 98.9247311827957\n",
      "epoch: 34, step: 0, loss: 0.022283880040049553\n",
      "epoch: 34, step: 200, loss: 0.031919412314891815\n",
      "epoch: 34, step: 400, loss: 0.09679806977510452\n",
      "Эпоха train: 34 Итераций: 490 train loss: 0.05802603298424011 train acc: 98.25379057698179\n",
      "epoch: 34, step: 0, loss: 3.217119683540659e-06\n",
      "Потрачено 14.5 минут на 34 эпоху\n",
      "Эпоха val: 34 Итераций: 22 val loss: 1.0774189835345686e-05 val acc: 98.9247311827957\n",
      "epoch: 35, step: 0, loss: 0.04898535832762718\n",
      "epoch: 35, step: 200, loss: 0.12349717319011688\n",
      "epoch: 35, step: 400, loss: 0.00483278650790453\n",
      "Эпоха train: 35 Итераций: 490 train loss: 0.05779855487950156 train acc: 98.2822835046301\n",
      "epoch: 35, step: 0, loss: 1.1920899822825959e-07\n",
      "Потрачено 14.5 минут на 35 эпоху\n",
      "Эпоха val: 35 Итераций: 22 val loss: 4.548783644864329e-06 val acc: 98.9247311827957\n",
      "epoch: 36, step: 0, loss: 0.1880112886428833\n",
      "epoch: 36, step: 200, loss: 0.029555978253483772\n",
      "epoch: 36, step: 400, loss: 0.056370094418525696\n",
      "Эпоха train: 36 Итераций: 490 train loss: 0.06118610255967835 train acc: 98.18052304874327\n",
      "epoch: 36, step: 0, loss: 6.079664416347441e-08\n",
      "Потрачено 14.6 минут на 36 эпоху\n",
      "Эпоха val: 36 Итераций: 22 val loss: 3.1122130841332307e-06 val acc: 98.9247311827957\n",
      "epoch: 37, step: 0, loss: 0.004798047710210085\n",
      "epoch: 37, step: 200, loss: 0.0025170836597681046\n",
      "epoch: 37, step: 400, loss: 0.0014226515777409077\n",
      "Эпоха train: 37 Итераций: 490 train loss: 0.05094855866577282 train acc: 98.48173399816831\n",
      "epoch: 37, step: 0, loss: 0.003214947646483779\n",
      "Потрачено 14.6 минут на 37 эпоху\n",
      "Эпоха val: 37 Итераций: 22 val loss: 0.00015664227225628294 val acc: 98.9247311827957\n",
      "epoch: 38, step: 0, loss: 0.038340285420417786\n",
      "epoch: 38, step: 200, loss: 0.043250832706689835\n",
      "epoch: 38, step: 400, loss: 0.008672711439430714\n",
      "Эпоха train: 38 Итераций: 490 train loss: 0.05212711152137728 train acc: 98.48987483463925\n",
      "epoch: 38, step: 0, loss: 9.131255751526623e-07\n",
      "Потрачено 14.6 минут на 38 эпоху\n",
      "Эпоха val: 38 Итераций: 22 val loss: 1.099447825417518e-06 val acc: 98.9247311827957\n",
      "epoch: 39, step: 0, loss: 0.030152253806591034\n",
      "epoch: 39, step: 200, loss: 0.04276508837938309\n",
      "epoch: 39, step: 400, loss: 0.011296852491796017\n",
      "Эпоха train: 39 Итераций: 490 train loss: 0.057529355845252546 train acc: 98.35962145110409\n",
      "epoch: 39, step: 0, loss: 7.819978122824978e-07\n",
      "Потрачено 14.6 минут на 39 эпоху\n",
      "Эпоха val: 39 Итераций: 22 val loss: 1.1355586397360733e-06 val acc: 98.9247311827957\n",
      "epoch: 40, step: 0, loss: 0.06381900608539581\n",
      "epoch: 40, step: 200, loss: 0.03460080921649933\n",
      "epoch: 40, step: 400, loss: 0.03920946270227432\n",
      "Эпоха train: 40 Итераций: 490 train loss: 0.05593324851723416 train acc: 98.39829042434111\n",
      "epoch: 40, step: 0, loss: 0.0\n",
      "Потрачено 14.7 минут на 40 эпоху\n",
      "Эпоха val: 40 Итераций: 22 val loss: 8.862666844855482e-07 val acc: 98.9247311827957\n",
      "epoch: 41, step: 0, loss: 0.023365039378404617\n",
      "epoch: 41, step: 200, loss: 0.0499294251203537\n",
      "epoch: 41, step: 400, loss: 0.05642318353056908\n",
      "Эпоха train: 41 Итераций: 490 train loss: 0.054337835120398324 train acc: 98.41050167904753\n",
      "epoch: 41, step: 0, loss: 8.833018227960565e-07\n",
      "Потрачено 14.8 минут на 41 эпоху\n",
      "Эпоха val: 41 Итераций: 22 val loss: 4.123711738937804e-06 val acc: 98.9247311827957\n",
      "epoch: 42, step: 0, loss: 0.002524062991142273\n",
      "epoch: 42, step: 200, loss: 0.09158189594745636\n",
      "epoch: 42, step: 400, loss: 0.08676449954509735\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []   \n",
    "\n",
    "y_true_t, y_true_v = [], []\n",
    "y_pred_t, y_pred_v = [], []\n",
    "\n",
    "for epoch in range(33, n_epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    train_class_correct = list(0. for i in range(classes1))\n",
    "    train_class_total = list(0. for i in range(classes1))\n",
    "    for i, batch in enumerate(data_loader_train12):\n",
    "        optimizer.zero_grad()\n",
    "        image, targets = batch['images'].to(device), batch['targets'].to(device)\n",
    "#         image = list(img.to(device) for img in image)\n",
    "#         targets = [t.to(device) for t in targets]\n",
    "        # Forward pass.\n",
    "        outputs = model(image)\n",
    "        # Calculate the loss.\n",
    "        loss = loss_func(outputs, targets)\n",
    "        train_running_loss += loss.item()\n",
    "        # Calculate the accuracy.\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_running_correct += (preds == targets).sum().item()\n",
    "        correct  = (preds == targets).squeeze()\n",
    "        for a in range(len(preds)):\n",
    "                label = targets[a]\n",
    "                # print(label)\n",
    "                train_class_correct[label] += correct[a].item()\n",
    "                train_class_total[label] += 1\n",
    "        # Backpropagation.\n",
    "        loss.backward()\n",
    "        # Update the weights.\n",
    "        optimizer.step()     \n",
    "        \n",
    "        y_true_t.extend([int(item) for item in targets])\n",
    "        y_pred_t.extend([int(item) for item in preds])\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "    \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    train_epoch_loss = train_running_loss / i\n",
    "    train_epoch_acc = 100. * (train_running_correct / len(df12))\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_epoch_accuracy = metrics.accuracy_score(y_true_t, y_pred_t)\n",
    "    train_epoch_f1_micro = metrics.f1_score(y_true_t, y_pred_t, average=\"micro\")\n",
    "    train_epoch_f1_macro =  metrics.f1_score(y_true_t, y_pred_t, average=\"macro\")\n",
    "    train_epoch_f1_weighted = metrics.f1_score(y_true_t, y_pred_t, average=\"weighted\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Эпоха train:', epoch,'Итераций:', i, 'train loss:', train_epoch_loss, 'train acc:', train_epoch_acc)\n",
    "    \n",
    "    model.eval()\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    # We need two lists to keep track of class-wise accuracy.\n",
    "    class_correct = list(0. for i in range(classes1))\n",
    "    class_total = list(0. for i in range(classes1))\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader_val12):\n",
    "#             image, targets = batch\n",
    "            image, targets = batch['images'].to(device), batch['targets'].to(device)\n",
    "#             image = list(img.to(device) for img in image)\n",
    "#             targets = [t.to(device) for t in targets]\n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            # Calculate the loss.\n",
    "            loss = loss_func(outputs, targets)\n",
    "            valid_running_loss += loss.item()\n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            valid_running_correct += (preds == targets).sum().item()\n",
    "            # Calculate the accuracy for each class.\n",
    "            correct  = (preds == targets).squeeze()\n",
    "            for j in range(len(preds)):\n",
    "                label = targets[j]\n",
    "                # print(label)\n",
    "                class_correct[label] += correct[j].item()\n",
    "                class_total[label] += 1\n",
    "        \n",
    "            if i % 200 == 0:\n",
    "                 print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "                    \n",
    "            y_true_v.extend([int(item) for item in targets])\n",
    "            y_pred_v.extend([int(item) for item in preds])\n",
    "        \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    valid_epoch_loss = valid_running_loss / i\n",
    "    valid_epoch_acc = 100. * (valid_running_correct / len(df120))\n",
    "    \n",
    "    valid_epoch_accuracy = metrics.accuracy_score(y_true_v, y_pred_v)\n",
    "    valid_epoch_f1_micro = metrics.f1_score(y_true_v, y_pred_v, average=\"micro\")\n",
    "    valid_epoch_f1_macro =  metrics.f1_score(y_true_v, y_pred_v, average=\"macro\")\n",
    "    valid_epoch_f1_weighted = metrics.f1_score(y_true_v, y_pred_v, average=\"weighted\")\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Потрачено {round((end - start) / 60, 1)} минут на {epoch} эпоху\")\n",
    "    print('Эпоха val:', epoch, 'Итераций:', i, 'val loss:', valid_epoch_loss, 'val acc:', valid_epoch_acc)\n",
    "    \n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    train_acc.append(train_epoch_acc)\n",
    "    valid_acc.append(valid_epoch_acc)\n",
    "    \n",
    "    torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'accuracy_train': train_epoch_accuracy,\n",
    "                    'f1_micro_trainl': train_epoch_f1_micro,\n",
    "                    'f1_macro_train': train_epoch_f1_macro,\n",
    "                    'f1_weighted_train': train_epoch_f1_weighted,\n",
    "                    'loss_train': train_loss,\n",
    "                    'acc_train': train_acc,\n",
    "                    'accuracy_val': valid_epoch_accuracy,\n",
    "                    'f1_micro_val': valid_epoch_f1_micro,\n",
    "                    'f1_macro_val': valid_epoch_f1_macro,\n",
    "                    'f1_weighted_val': valid_epoch_f1_weighted,\n",
    "                    'loss_val': valid_loss,\n",
    "                    'acc_val': valid_acc\n",
    "                    }, f'./chkpt_modelrv3_clf_{epoch}.pth')\n",
    "    \n",
    "    torch.save(model.state_dict(), f'./chkpt_mrv3_clf_{epoch}.pth')\n",
    "    !cp chkpt_mrv3_clf_{epoch}.pth /content/drive/MyDrive\n",
    "    !cp chkpt_modelrv3_clf_{epoch}.pth /content/drive/MyDrive"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "011c65e2a39e4ef78d179fb576e62b50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11b85256ae634a7f8c5888b9d95f7fab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84281c902e854c3f8d2837bd0f403973",
      "placeholder": "​",
      "style": "IPY_MODEL_c7a43655801c4f26b0ea17abdbd4ce86",
      "value": "100%"
     }
    },
    "1966adf0227d476ba2d2fde8667a59a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_972952feb44344a3a6fc5f3f57213c35",
      "placeholder": "​",
      "style": "IPY_MODEL_b03f62472eed4894875f248cc9d2e0ec",
      "value": " 5000/5000 [00:00&lt;00:00, 23293.22it/s]"
     }
    },
    "2931c0be4190463a9737dfd1014d1c65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5605abfb61e74085b67c4a9038bd0e99",
      "max": 54188,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e54368ddea0b4e5bb6b4069212a59234",
      "value": 54188
     }
    },
    "4015d9d2b89745f7a0e5c02f1b831e46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7efb490b06a6468a9eacc565590e40e0",
      "max": 5000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a041ca87041a4c879378a6367a94b5c8",
      "value": 5000
     }
    },
    "4699f982125c424eab3f66f7cc568fda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49d60872db5b4e6b93f8a17a49621dc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5605abfb61e74085b67c4a9038bd0e99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e7f57c9a4ca48039a74946534b93b60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fa2910752c1474db6e264390c164476": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7efb490b06a6468a9eacc565590e40e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84281c902e854c3f8d2837bd0f403973": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95d4d630b9e04eae824464cea9a89045": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "972952feb44344a3a6fc5f3f57213c35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a041ca87041a4c879378a6367a94b5c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a1a5cd0294214b1ba95fe99f3f945caf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11b85256ae634a7f8c5888b9d95f7fab",
       "IPY_MODEL_4015d9d2b89745f7a0e5c02f1b831e46",
       "IPY_MODEL_1966adf0227d476ba2d2fde8667a59a3"
      ],
      "layout": "IPY_MODEL_5e7f57c9a4ca48039a74946534b93b60"
     }
    },
    "b03f62472eed4894875f248cc9d2e0ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd410b7767754f6fab4959c5f0c2a7e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95d4d630b9e04eae824464cea9a89045",
      "placeholder": "​",
      "style": "IPY_MODEL_49d60872db5b4e6b93f8a17a49621dc1",
      "value": "100%"
     }
    },
    "c230bd6adc914d9a91890b0dbc2d0110": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4699f982125c424eab3f66f7cc568fda",
      "placeholder": "​",
      "style": "IPY_MODEL_5fa2910752c1474db6e264390c164476",
      "value": " 54188/54188 [00:03&lt;00:00, 18003.71it/s]"
     }
    },
    "c7a43655801c4f26b0ea17abdbd4ce86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e54368ddea0b4e5bb6b4069212a59234": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fcf0e6fd5998483fac562f3ccbdd91e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd410b7767754f6fab4959c5f0c2a7e9",
       "IPY_MODEL_2931c0be4190463a9737dfd1014d1c65",
       "IPY_MODEL_c230bd6adc914d9a91890b0dbc2d0110"
      ],
      "layout": "IPY_MODEL_011c65e2a39e4ef78d179fb576e62b50"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
